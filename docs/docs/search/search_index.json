{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with Lemonade Server","text":"<p>\ud83c\udf4b Lemonade Server is a server interface that uses the standard Open AI API, allowing applications to integrate with local LLMs. This means that you can easily replace cloud-based LLMs with private and free LLMs that run locally on your own PC's NPU and GPU.</p> <p>Lemonade Server is available as a standalone tool with a one-click Windows GUI installer.</p> <p>Once you've installed, we recommend checking out these resources:</p> Documentation Description Supported Applications Explore applications that work out-of-the-box with Lemonade Server. Lemonade Server Concepts Background knowledge about local LLM servers and the OpenAI standard. <code>lemonade-server</code> CLI Guide Learn how to manage the server process and install new models using the command-line interface. Models List Browse a curated set of LLMs available for serving. Server Spec Review all supported OpenAI-compatible and Lemonade-specific API endpoints. Integration Guide Step-by-step instructions for integrating Lemonade Server into your own applications. <p>Note: if you want to develop Lemonade Server itself, you can install from source.</p>"},{"location":"#integrate-lemonade-server-with-your-application","title":"Integrate Lemonade Server with Your Application","text":"<p>Since Lemonade Server implements the standard OpenAI API specification, you can use any OpenAI-compatible client library by configuring it to use <code>http://localhost:8000/api/v1</code> as the base URL. A table containing official and popular OpenAI clients on different languages is shown below.</p> <p>Feel free to pick and choose your preferred language.</p> Python C++ Java C# Node.js Go Ruby Rust PHP openai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php"},{"location":"#python-client-example","title":"Python Client Example","text":"<pre><code>from openai import OpenAI\n\n# Initialize the client to use Lemonade Server\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\",\n    api_key=\"lemonade\"  # required but unused\n)\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",  # or any other available model\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\n# Print the response\nprint(completion.choices[0].message.content)\n</code></pre> <p>For more detailed integration instructions, see the Integration Guide.</p>"},{"location":"contribute/","title":"Lemonade SDK Contribution Guide","text":"<p>The Lemonade SDK project welcomes contributions from everyone!</p> <p>See code organization for an overview of the repository.</p>"},{"location":"contribute/#collaborate-with-your-app","title":"Collaborate with Your App","text":"<p>Lemonade Server integrates quickly with most OpenAI-compatible LLM apps.</p> <p>You can: - Share an example of your app using Lemonade via Discord, GitHub Issue, or email. - Contribute a guide by adding a <code>.md</code> file to the server apps folder. Follow the style of the Open WebUI guide.</p> <p>Guides should: - Work in under 10 minutes. - Require no code changes to the app. - Use OpenAI API-compatible apps with configurable base URLs.</p>"},{"location":"contribute/#sdk-contributions","title":"SDK Contributions","text":"<p>To contribute code or examples, first open an Issue with:    - A descriptive title.    - Relevant labels (<code>enhancement</code>, <code>good first issue</code>, etc.).    - A proposal explaining what you're contributing.    - The use case it supports.</p> <p>One of the maintainers will get back to you ASAP with guidance.</p>"},{"location":"contribute/#issues","title":"Issues","text":"<p>Use Issues to report bugs or suggest features. </p> <p>A maintainer will apply one of these labels to indicate the status: - <code>on roadmap</code>: planned for development. - <code>good first issue</code>: open for contributors. - <code>needs detail</code>: needs clarification before proceeding. - <code>wontfix</code>: out of scope or unmaintainable.</p>"},{"location":"contribute/#pull-requests","title":"Pull Requests","text":"<p>Submit a PR to contribute code. Maintainers: - @danielholanda - @jeremyfowers - @ramkrishna - @vgodsoe</p> <p>Discuss major changes via an Issue first.</p>"},{"location":"contribute/#testing","title":"Testing","text":"<p>Tests are run automatically on each PR. These include: - Linting - Code formatting (<code>black</code>) - Unit tests - End-to-end tests</p> <p>To run tests locally, use the commands in <code>.github/workflows/</code>.</p>"},{"location":"contribute/#versioning","title":"Versioning","text":"<p>We follow Semantic Versioning.</p>"},{"location":"faq/","title":"\ud83c\udf4b Lemonade Frequently Asked Questions","text":""},{"location":"faq/#overview","title":"Overview","text":""},{"location":"faq/#1-what-is-lemonade-sdk-and-what-does-it-include","title":"1. What is Lemonade SDK and what does it include?","text":"<p>Lemonade is an open-source SDK that provides high-level APIs, CLI tools, and a server interface to deploy and benchmark LLMs using ONNX Runtime GenAI (OGA), Hugging Face Transformers, and llama.cpp backends.</p>"},{"location":"faq/#2-what-is-lemonade-server-and-how-is-it-different-from-the-sdk","title":"2. What is Lemonade Server and how is it different from the SDK?","text":"<p>Lemonade Server is a component of the SDK that enables local LLM deployment via an OpenAI-compatible API. It allows integration with apps like chatbots and coding assistants without requiring code changes. It's available as a standalone Windows GUI installer or via command line for Linux.</p>"},{"location":"faq/#3-what-are-the-use-cases-for-different-audiences","title":"3. What are the use cases for different audiences?","text":"<ul> <li>End Users: Use GAIA for a Chatbot experience locally.</li> <li>LLM Enthusiasts: LLMs on your GPU or NPU with minimal setup, and connect to great apps listed here.</li> <li>Developers: Integrate LLMs into apps using standard APIs with no device-specific code. See the Server Integration Guide.</li> </ul>"},{"location":"faq/#installation-compatibility","title":"Installation &amp; Compatibility","text":""},{"location":"faq/#1-how-do-i-install-lemonade-sdk-or-server","title":"1. How do I install Lemonade SDK or Server?","text":"<p>Visit https://lemonade-server.ai/install_options.html and click the options that apply to you.</p>"},{"location":"faq/#2-which-devices-are-supported","title":"2. Which devices are supported?","text":"<p>\ud83d\udc49 Supported Configurations</p> <p>For more information on Hybrid/NPU Support, see the section Hybrid/NPU.</p>"},{"location":"faq/#3-is-linux-supported","title":"3. Is Linux supported?","text":"<p>Yes! To install Lemonade on Linux, visit https://lemonade-server.ai/ and check the \"Developer Setup\" section for installation instructions. Visit the Supported Configurations section to see the support matrix for CPU, GPU, and NPU.</p>"},{"location":"faq/#4-how-do-i-uninstall-lemonade-server-windows","title":"4. How do I uninstall Lemonade Server? (Windows)","text":"<p>To completely uninstall Lemonade Server from your system, follow these steps:</p> <p>Step 1: Remove cached files    - Open File Explorer and navigate to <code>%USERPROFILE%\\.cache</code>    - Delete the <code>lemonade</code> folder if it exists    - [Optional] To remove downloaded models, delete the <code>huggingface</code> folder</p> <p>Step 2: Remove from PATH environment variable    - Press <code>Win + I</code> to open Windows Settings    - Search for \"environment variables\" and select \"Edit environment variables for your account\"    - Find \"Path\" in the list and click \"Edit\"    - Look for the entry containing <code>lemonade_server\\bin</code> and select it    - Click \"Delete\" then \"OK\"</p> <p>Step 3: Delete installation folder    - Navigate to <code>%LOCALAPPDATA%\\lemonade_server</code>    - Delete the entire <code>lemonade_server</code> folder</p>"},{"location":"faq/#models-performance","title":"Models &amp; Performance","text":""},{"location":"faq/#1-what-models-are-supported","title":"1. What models are supported?","text":"<p>Lemonade supports a wide range of LLMs including LLaMA, DeepSeek, Qwen, Gemma, Phi, and gpt-oss. Most GGUF models can also be added to Lemonade Server by users using the Model Manager interface.</p> <p>\ud83d\udc49 Supported Models List</p>"},{"location":"faq/#2-how-do-i-know-what-size-model-will-work-with-my-setup","title":"2. How do I know what size model will work with my setup?","text":"<p>Model compatibility depends on your system's RAM, VRAM, and NPU availability. The actual file size varies significantly between models due to different quantization techniques and architectures.</p> <p>To check if a model will work:    1. Visit the model's Hugging Face page (e.g., <code>amd/Qwen2.5-7B-Chat-awq-g128-int4-asym-fp16-onnx-hybrid</code>).    2. Check the \"Files and versions\" tab to see the actual download size.    3. Add ~2-4 GB overhead for KV cache, activations, and runtime memory.    4. Ensure your system has sufficient RAM/VRAM.</p>"},{"location":"faq/#3-im-looking-for-a-model-but-its-not-listed-in-the-model-manager","title":"3. I'm looking for a model, but it's not listed in the Model Manager.","text":"<p>If a model isn't listed, it may not yet be validated or compatible with your selected backend (for example, Hybrid models will not show if Ryzen AI Hybrid software is not installed). You can:</p> <ul> <li>Add a custom model manually via the Lemonade Server Model Manager's \"Add a Model\" interface.</li> <li>Request support by opening a GitHub issue.</li> </ul>"},{"location":"faq/#4-is-there-a-script-or-tool-to-convert-models-to-hybrid-format","title":"4. Is there a script or tool to convert models to hybrid format?","text":"<p>Yes, there's a guide on preparing your models for Ryzen AI NPU:</p> <p>\ud83d\udc49 Model Preparation Guide</p>"},{"location":"faq/#5-whats-the-difference-between-gguf-and-onnx-models","title":"5. What's the difference between GGUF and ONNX models?","text":"<ul> <li>GGUF: Used with llama.cpp backend, supports CPU, and GPU via Vulkan or ROCm.</li> <li>ONNX: Used with OnnxRuntime GenAI, supports NPU and NPU+iGPU Hybrid execution.</li> </ul>"},{"location":"faq/#inference-behavior-performance","title":"Inference Behavior &amp; Performance","text":""},{"location":"faq/#1-what-are-the-performance-benchmarks-that-can-be-collected-using-lemonade-sdk","title":"1. What are the performance benchmarks that can be collected using Lemonade SDK?","text":"<p>You can measure:</p> <ul> <li>Inference speed</li> <li>Time to first token</li> <li>Tokens per second</li> <li>Accuracy via MMLU and other benchmarks</li> </ul>"},{"location":"faq/#2-can-lemonade-print-out-stats-like-tokens-per-second","title":"2. Can Lemonade print out stats like tokens per second?","text":"<p>Yes! Lemonade Server exposes a <code>/stats</code> endpoint that returns performance metrics from the most recent completion request:</p> <pre><code>curl http://localhost:8000/api/v1/stats\n</code></pre> <p>Or, you can launch <code>lemonade-server</code> with the option <code>--log-level debug</code> and that will also print out stats.</p>"},{"location":"faq/#3-how-does-lemonades-performance-compare-to-llamacpp","title":"3. How does Lemonade's performance compare to llama.cpp?","text":"<p>Lemonade supports llama.cpp as a backend, so performance is similar when using the same model and quantization.</p>"},{"location":"faq/#hybrid-and-npu-questions","title":"Hybrid and NPU Questions","text":""},{"location":"faq/#1-does-hybrid-inference-with-the-npu-only-work-on-windows","title":"1. Does hybrid inference with the NPU only work on Windows?","text":"<p>Yes, hybrid inference is currently supported only on Windows. NPU-only inference is coming to Linux soon, followed by hybrid (NPU+iGPU) support via ROCm.</p>"},{"location":"faq/#2-i-loaded-a-hybrid-model-but-the-npu-is-barely-active-is-that-expected","title":"2. I loaded a hybrid model, but the NPU is barely active. Is that expected?","text":"<p>Yes. In hybrid mode:</p> <ul> <li>The NPU handles prompt processing.</li> <li>The GPU handles token generation.</li> <li>If your prompt is short, the NPU finishes quickly. Try a longer prompt to see more NPU activity.</li> </ul>"},{"location":"faq/#3-does-lemonade-work-on-older-amd-processors-or-non-ryzen-ai-systems","title":"3. Does Lemonade work on older AMD processors or non-Ryzen AI systems?","text":"<p>Yes! Lemonade supports multiple execution modes:</p> <ul> <li>AMD Ryzen 7000/8000/200 series: GPU acceleration via llama.cpp + Vulkan backend</li> <li>Systems with Radeon GPUs: Yes</li> <li>Any x86 CPU: Yes</li> <li>Intel/NVIDIA systems: CPU inference, with GPU support if compatible drivers are available</li> </ul> <p>While you won't get NPU acceleration on non-Ryzen AI 300 systems, you can still benefit from GPU acceleration and the OpenAI-compatible API.</p>"},{"location":"faq/#4-how-do-i-know-what-model-architectures-are-supported-by-the-npu","title":"4. How do I know what model architectures are supported by the NPU?","text":"<p>AMD publishes pre-quantized and optimized models in their Hugging Face collections:</p> <ul> <li>Ryzen AI NPU Models</li> <li>Ryzen AI Hybrid Models</li> </ul> <p>To find the architecture of a specific model, click on any model in these collections and look for the \"Base model\" field, which will show you the underlying architecture (e.g., Llama, Qwen, Phi).</p>"},{"location":"faq/#5-how-can-i-get-better-performance-from-the-npu","title":"5. How can I get better performance from the NPU?","text":"<p>Make sure that you've put the NPU in \"Turbo\" mode to get the best results. This is done by opening a terminal window and running the following commands:</p> <pre><code>cd C:\\Windows\\System32\\AMD\n.\\xrt-smi configure --pmode turbo\n</code></pre>"},{"location":"faq/#support-roadmap","title":"Support &amp; Roadmap","text":""},{"location":"faq/#1-what-if-i-encounter-installation-or-runtime-errors","title":"1. What if I encounter installation or runtime errors?","text":"<p>Check the Lemonade Server logs via the tray icon. Common issues include model compatibility or outdated versions.</p> <p>\ud83d\udc49 Open an Issue on GitHub</p>"},{"location":"faq/#2-lemonade-is-missing-a-feature-i-really-want-what-should-i-do","title":"2. Lemonade is missing a feature I really want. What should I do?","text":"<p>Open a feature request on GitHub. We're actively shaping the roadmap based on user feedback.</p>"},{"location":"faq/#3-do-you-plan-to-share-a-roadmap","title":"3. Do you plan to share a roadmap?","text":"<p>Yes! We tag roadmap items on GitHub with the \"on roadmap\" label.</p> <p>\ud83d\udc49 Lemonade SDK Roadmap Issues</p>"},{"location":"lemonade_api/","title":"\ud83c\udf4b Lemonade API: Model Compatibility and Recipes","text":"<p>Lemonade API (<code>lemonade.api</code>) provides a simple, high-level interface to load and run LLM models locally. This guide helps you understand what models work with which recipes, what to expect in terms of compatibility, and how to choose the right setup for your hardware.</p>"},{"location":"lemonade_api/#what-is-a-recipe","title":"\ud83e\udde0 What Is a Recipe?","text":"<p>A recipe defines how a model is run \u2014 including backend (e.g., PyTorch, ONNX Runtime), quantization strategy, and device support. The <code>from_pretrained()</code> function in <code>lemonade.api</code> uses the recipe to configure everything automatically. For the list of recipes, see Recipe Compatibility Table. The following is an example of using the Lemonade API <code>from_pretrained()</code> function:</p> <pre><code>from lemonade.api import from_pretrained\n\nmodel, tokenizer = from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", recipe=\"hf-cpu\")\n</code></pre> <p>Function Arguments: - checkpoint: The Hugging Face or OGA checkpoint that defines the LLM. - recipe: Defines the implementation and hardware used for the LLM. Default is \"hf-cpu\".</p>"},{"location":"lemonade_api/#system-information-functions","title":"\ud83d\udd0d System Information Functions","text":"<p>The Lemonade API also provides functions to programmatically access system and device information:</p> <pre><code>from lemonade.api import get_system_info, get_device_info\n\n# Get essential system information (OS, processor, memory, devices)\nsystem_info = get_system_info()\n\n# Get detailed system information including Python packages and extended details\nsystem_info_verbose = get_system_info(verbose=True)\n\n# Get only device information (CPU, GPU, NPU details)\ndevice_info = get_device_info()\n</code></pre> <p>These functions return the same information available through the <code>lemonade system-info</code> CLI command and the <code>/api/v1/system-info</code> server endpoint, including: - Hardware details (CPU, memory, BIOS) - Device availability (AMD iGPU, dGPU, NPU) - Inference engine compatibility per device - Driver versions and system configuration - Python package versions (verbose mode only)</p>"},{"location":"lemonade_api/#supported-model-formats","title":"\ud83d\udcdc Supported Model Formats","text":"<p>Lemonade API currently supports:</p> <ul> <li>Hugging Face hosted safetensors checkpoints</li> <li>AMD OGA (ONNXRuntime-GenAI) ONNX checkpoints</li> </ul> <p></p>"},{"location":"lemonade_api/#recipe-and-checkpoint-compatibility","title":"\ud83c\udf74 Recipe and Checkpoint Compatibility","text":"<p>The following table explains what checkpoints work with each recipe, the hardware and OS requirements, and additional notes:</p> Recipe Checkpoint Format Hardware Needed Operating System Notes <code>hf-cpu</code> safetensors (Hugging Face) Any x86 CPU Windows, Linux Compatible with x86 CPUs, offering broad accessibility. <code>hf-dgpu</code> safetensors (Hugging Face) Compatible Discrete GPU Windows, Linux Requires PyTorch and a compatible GPU.<sup>[1]</sup> <code>oga-cpu</code> safetensors (Hugging Face) Any x86 CPU Windows Converted from safetensors via `model_builder`. Accuracy loss due to RTN quantization. OGA ONNX Any x86 CPU Windows Use models from the CPU Collection. OGA ONNX AMD Ryzen AI PC Windows Use models from the GPU Collection. <code>oga-hybrid</code> Pre-quantized OGA ONNX AMD Ryzen AI 300 series PC Windows Use models from the Hybrid Collection. Optimized with AWQ to INT4. <code>oga-npu</code> Pre-quantized OGA ONNX AMD Ryzen AI 300 series PC Windows Use models from the NPU Collection. Optimized with AWQ to INT4. <p><sup>[1]</sup> Compatible GPUs are those that support PyTorch's <code>.to(\"cuda\")</code> function. Ensure you have the appropriate version of PyTorch installed (e.g., CUDA or ROCm) for your specific GPU. Note: Lemonade does not install PyTorch with CUDA or ROCm for you. For installation instructions, see PyTorch's Get Started Guide.</p>"},{"location":"lemonade_api/#converting-models-to-oga","title":"\ud83d\udd04 Converting Models to OGA","text":"<p>Lemonade API will do the conversion for you using OGA's <code>model_builder</code> if you pass a safetensors checkpoint.</p> <ul> <li>Takes \\~1\u20135 minutes per model.</li> <li>Uses RTN quantization (int4).</li> <li>For better quality, use pre-quantized models (see below).</li> </ul>"},{"location":"lemonade_api/#pre-converted-oga-models","title":"\ud83d\udce6 Pre-Converted OGA Models","text":"<p>You can skip the conversion step by using pre-quantized models from AMD\u2019s Hugging Face collection. These models are optimized using Activation Aware Quantization (AWQ), which provides higher-accuracy int4 quantization compared to RTN.</p> Recipe Collection <code>oga-hybrid</code> Hybrid Collection <code>oga-npu</code> NPU Collection <code>oga-cpu</code> CPU Collection"},{"location":"lemonade_api/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Lemonade API Examples</li> <li>lemonade.api source</li> <li>Model Support Matrix (ONNX Runtime GenAI)</li> </ul>"},{"location":"self_hosted_runners/","title":"\ud83c\udf29\ufe0f Self Hosted Runners \ud83c\udf29\ufe0f Documentation","text":"<p>This page documents how to set up and maintain self-hosted runners for lemonade-sdk.</p> <p>Topics:  - What are Self-Hosted Runners?  - NPU Runner Setup  - Maintenance and Troubleshooting     - Check your runner's status     - Actions are failing unexpectedly     - Take a laptop offline - Creating Workflows     - Capabilities and Limitations</p>"},{"location":"self_hosted_runners/#what-are-self-hosted-runners","title":"What are Self-Hosted Runners?","text":"<p>A \"runner\" is a computer that has installed GitHub's runner software, which runs a service that makes the laptop available to run GitHub Actions. In turn, Actions are defined by Workflows, which specify when the Action should run (manual trigger, CI, CD, etc.) and what the Action does (run tests, build packages, run an experiment, etc.). </p> <p>You can read about all this here: GitHub: About self-hosted runners.</p>"},{"location":"self_hosted_runners/#npu-runner-setup","title":"NPU Runner Setup","text":"<p>This guide will help you set up a Ryzen AI laptop as a GitHub self-hosted runner. This will make the laptop available for on-demand and CI jobs that require NPU resources.</p>"},{"location":"self_hosted_runners/#new-machine-setup","title":"New Machine Setup","text":"<ul> <li>Install the following software:<ul> <li>The latest RyzenAI driver ONLY (do not install RyzenAI Software), which is available here</li> <li>VS Code</li> <li>git</li> </ul> </li> <li>If your laptop has an Nvidia GPU, you must disable it in device manager</li> <li>Open a PowerShell script in admin mode, and run <code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned</code></li> <li>Go into Windows settings:</li> <li>Go to system, power &amp; battery, screen sleep &amp; hibernate timeouts, and make it so the laptop never sleeps while plugged in. If you don't do this it can fall asleep during jobs.</li> <li>Searrch \"Change the date and time\", and then click \"sync\" under \"additional settings.\"</li> </ul>"},{"location":"self_hosted_runners/#runner-configuration","title":"Runner Configuration","text":"<p>These steps will place your machine in the <code>stx-test</code> pool, which is where we put machines while we are setting them up. In the next section we will finalize setup and then move the runner into the production pool.</p> <ol> <li> <p>IMPORTANT: before doing step 2, read this: </p> <ul> <li>Use a powershell administrator mode terminal</li> <li>Enable permissions by running <code>Set-ExecutionPolicy RemoteSigned</code></li> <li>When running <code>./config.cmd</code> in step 2, make the following choices:<ul> <li>Name of the runner group = <code>stx</code></li> <li>For the runner name, call it <code>NAME-stx-NUMBER</code>, where NAME is your alias and NUMBER would tell you this is the Nth STX machine you've added.</li> <li>Apply the label <code>stx-test</code> as well as a label with your name to indicate that you are maintaining the runner.</li> <li>Accept the default for the work folder</li> <li>You want the runner to function as a service (respond Y)</li> <li>User account to use for the service = <code>NT AUTHORITY\\SYSTEM</code> (not the default of <code>NT AUTHORITY\\NETWORK SERVICE</code>)</li> </ul> </li> </ul> </li> <li> <p>Follow the instructions here for Windows|Ubuntu, minding what we said in step 1: https://github.com/organizations/lemonade-sdk/settings/actions/runners/new</p> </li> <li>You should see your runner show up in the <code>stx</code> runner group in the lemonade-sdk org</li> </ol>"},{"location":"self_hosted_runners/#runner-setup","title":"Runner Setup","text":"<p>These steps will use GitHub Actions to run automated setup and validation for your new runner while it is still in the <code>stx-test</code> group.</p> <ol> <li>Go to the lemonade ryzenai test action and click \"run workflow\".<ul> <li>Select <code>stx-test</code> as the nimbys group</li> <li>Check the box for \"Install miniforge\"</li> <li>Click <code>Run workflow</code></li> </ul> </li> <li>The workflow should appear at the top of the queue. Click into it.<ul> <li>Expand the <code>Set up job</code> section and make sure <code>Runner name:</code> refers to your new runner. Otherwise, the job may have gone to someone else's runner in the test group. You can re-queue the workflow until it lands on your runner.</li> <li>Wait for the workflow to finish successfully.</li> </ul> </li> <li>In a powershell admin terminal, run <code>Stop-Service \"actions.runner.*\"</code> and then <code>Start-Service \"actions.runner.*\"</code>. If you don't do this, the runner wont be able to find Conda.</li> <li>Repeat step 1, except do not check the box for \"Install miniforge\". Wait for it to finish successfully. Congrats, your new runner is working!</li> <li>Go to the stx Runner Group, click your new runner, and click the gear icon to change labels. Uncheck <code>stx-test</code> and check <code>stx</code>.</li> <li>Done!</li> </ol>"},{"location":"self_hosted_runners/#maintenance-and-troubleshooting","title":"Maintenance and Troubleshooting","text":"<p>This is a production system and things will go wrong. Here is some advice on what to do.</p>"},{"location":"self_hosted_runners/#check-your-runners-status","title":"Check your runner's status","text":"<p>You can run <code>Get-EventLog -LogName Application -Source ActionsRunnerService</code> in a powershell terminal on your runner to get more information about what it's been up to.</p> <p>If there have been any problems recently, they may show up like:</p> <ul> <li>Error: Runner connect error: &lt; details about the connection error &gt;</li> <li>Information: Runner reconnected</li> <li>Information: Running Job: &lt; job name &gt;</li> <li>Information: Jos &lt; job name &gt; completed with result: [Succeeded / Canceled / Failed]</li> </ul>"},{"location":"self_hosted_runners/#actions-are-failing-unexpectedly","title":"Actions are failing unexpectedly","text":"<p>Actions fail all the time, often because they are testing buggy code. However, sometimes an Action will fail because something is wrong with the specific NIMBYS runner that ran the Action. </p> <p>If this happens to you, here are some steps you can take (in order): 1. Take note of which runner executed your Action. You can check this by going to the <code>Set up job</code> section of the Action's log and checking the <code>Runner name:</code> field. The machine name in that field will correspond to a machine on the runners page. 1. Re-queue your job. It is possible that that the failure is a one-off, and it will work the next time on the same runner. Re-queuing also gives you a chance of getting a runner that is in a healthier state. 1. If the same runner is consistently failing, it is probably in an unhealthy state (or you have a bug in your code and you're just blaming the runner). If a runner is in an unhealthy state:     1. Take the laptop offline so that it stops being allocated Actions.     1. Open an Issue. Assign it to the maintainer of the laptop (their name should be in the runner's name). Link the multiple failed workflows that have convinced you that this runner is unhealthy.     1. Re-queue your job. You'll definitely get a different runner now since you took the unhealthy runner offline. 1. If all runners are consistently failing your workflow, seriously think about whether your code is the problem.</p>"},{"location":"self_hosted_runners/#take-a-laptop-offline","title":"Take a laptop offline","text":"<p>If you need to do some maintenance on your laptop, use it for dev/demo work, etc. you can remove it from the runners pool.</p> <p>Also, if someone else's laptop is misbehaving and causing Actions to fail unexpectedly, you can remove that laptop from the runners pool to make sure that only healthy laptops are selected for work.</p> <p>There are three options:</p> <p>Option 1, which is available to anyone in the <code>lemonade-sdk</code> org: remove the <code>stx</code> label from the runner. - Workflows use <code>runs-on: stx</code> to target runners with the <code>stx</code> label. Removing this label from the runner will thus remove the runner from the pool. - Go to the runners page, click the specific runner in question, click the gear icon in the Labels section, and uncheck <code>stx</code>. - To reverse this action later, go back to the runners page, click the gear icon, and check <code>stx</code>.</p> <p>Option 2, which requires physical/remote access to the laptop: - In a PowerShell terminal, run <code>Stop-Service \"actions.runner.*\"</code>. - To reverse this action, run <code>Start-Service \"actions.runner.*\"</code>.</p> <p>Option 3 is to just turn the laptop off :)</p>"},{"location":"self_hosted_runners/#creating-workflows","title":"Creating Workflows","text":"<p>GitHub Workflows define the Actions that run on self-hosted laptops to perform testing and experimentation tasks. This section will help you learn about what capabilities are available and show some examples of well-formed workflows.</p>"},{"location":"self_hosted_runners/#capabilities-and-limitations","title":"Capabilities and Limitations","text":"<p>Because we use self-hosted systems, we have to be careful about what we put into these workflows so that we avoid: - Corrupting the laptops, causing them to produce inconsistent results or failures. - Over-subscribing the capacity of the available laptops</p> <p>Here are some general guidelines to observe when creating or modifying workflows. If you aren't confident that you are properly following these guidelines, please contact someone to review your code before opening your PR.</p> <ul> <li>Place a \ud83c\udf29\ufe0f emoji in the name of all of your self-host workflows, so that PR reviewers can see at a glance which workflows are using self-hosted resources.<ul> <li>Example: <code>name: Test Lemonade on NPU and Hybrid with OGA environment \ud83c\udf29\ufe0f</code></li> </ul> </li> <li>Avoid triggering your workflow before anyone has had a chance to review it against these guidelines. To avoid triggers, do not include <code>on: pull request:</code> in your workflow until after a reviewer has signed off.</li> <li>Only map a workflow with <code>runs on: stx</code> if it actually requires Ryzen AI compute. If a step in your workflow can use generic compute (e.g., running a Hugging Face LLM on CPU), put that step on a generic non-NIMBYS runner like <code>runs on: windows-latest</code>.</li> <li>Be very considerate about installing software on to the runners:<ul> <li>Installing software into the CWD (e.g., a path of <code>.\\</code>) is always ok, because that will end up in <code>C:\\actions-runner\\_work\\REPO</code>, which is always wiped between tests.</li> <li>Installing software into <code>AppData</code>, <code>Program Files</code>, etc. is not advisable because that software will persist across tests. See the setup section to see which software is already expected on the system.</li> </ul> </li> <li>Always create new conda environments in the CWD, for example <code>conda create -p .\\my-env</code>.<ul> <li>This way, the conda environment is located in <code>C:\\actions-runner\\_work\\REPO</code>, which is wiped between tests.</li> <li>Do NOT create conda environments by name, for example <code>conda create -n dont-you-dare</code> since that will end up in the conda install location and will persist across tests.</li> <li>Make sure to activate your conda environment (e.g., <code>conda activate .\\lemon-npu-ci</code>) before running any <code>pip install</code> commands. Otherwise your workflow will modify the base environment!</li> </ul> </li> <li>PowerShell scripts do not necessarily raise errors by programs they call.<ul> <li>That means PowerShell can call a Python test, and then keep going and claim \"success\" even if that Python test fails and raises an error (non-zero exit code).</li> <li>You can add <code>if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }</code> after any line of script where it is that is particularly important to fail the workflow if the program in the preceding line raised an error.<ul> <li>For example, this will make sure that lemonade installed correctly: <ol> <li>pip install -e .</li> <li>if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }</li> </ol> </li> </ul> </li> </ul> </li> <li>Be considerate of how long your workflow will run for, and how often it will be triggered.<ul> <li>All workflows go into the same queue and share the same pool of runners.</li> <li>A good target length for a workflow is 15 minutes.</li> </ul> </li> <li>Be considerate of how much data your workflow will download.<ul> <li>It would be very bad to fill up a hard drive, since Windows machines misbehave pretty bad when their drives are full.</li> <li>Place your Hugging Face cache inside the <code>_work</code> directory so that it will be wiped after each job.<ul> <li>Example: <code>$Env:HF_HOME=\".\\hf-cache\"</code></li> </ul> </li> <li>Place your Lemonade cache inside the <code>_work</code> directory so that it will be wiped after each job.<ul> <li>Example: <code>lemonade -d .\\ci-cache</code> or <code>$Env:LEMONADE_CACHE_DIR=\".\\ci-cache\"</code>. Use the environment variable, rather than the <code>-d</code> flag, wherever possible since it will apply to all lemonade calls within the job step.</li> </ul> </li> </ul> </li> </ul>"},{"location":"self_hosted_runners/#license","title":"License","text":"<p>Apache 2.0 License</p> <p>Copyright(C) 2024-2025 Advanced Micro Devices, Inc. All rights reserved. SPDX-License-Identifier: MIT</p>"},{"location":"dev_cli/finetuned_model_export/","title":"Bring Your Own Finetuned Model Guide","text":"<p>Bring your own finetuned model into the Ryzen\u2122 AI ecosystem and run it locally! This guide walks you through the steps to quantize your model and export it for optimized performance with ONNX Runtime GenAI on AMD Ryzen AI devices.</p> <p>Workflow Summary:</p> <ol> <li>Finetune your model (using your preferred framework)</li> <li>Quantize using Quark's PTQ flow</li> <li>Export and run the quantized model using Lemonade, then deploy it with ONNX Runtime GenAI on Ryzen\u2122 AI-enabled devices.</li> </ol>"},{"location":"dev_cli/finetuned_model_export/#step-1-quantize-your-finetuned-model-with-quark","title":"Step 1: Quantize Your Finetuned Model with Quark","text":""},{"location":"dev_cli/finetuned_model_export/#11-install-quark","title":"1.1 Install Quark","text":"<ul> <li>Follow the Quark installation guide to download and set up Quark.</li> <li> <p>It is highly recommended to use GPU acceleration for quantizing LLMs, as CPU-based quantization can be time-consuming. Refer to the PyTorch GPU support section for setup instructions.</p> </li> <li> <p>Quark provides a pre-prepared examples folder to simplify the quantization process. Download the latest zip file matching your Quark version from the previous versions section. After extracting the folder, navigate to <code>examples/torch/language_modeling/llm_ptq</code> to find <code>quantize_quark.py</code>.</p> </li> <li> <p>For detailed instructions on using PTQ (Post-Training Quantization) with Quark, refer to the Quark PyTorch LLM PTQ example.</p> </li> </ul>"},{"location":"dev_cli/finetuned_model_export/#12-prepare-your-model","title":"1.2 Prepare Your Model","text":"<ul> <li>Ensure your finetuned model checkpoint is available locally (e.g., <code>[MODEL_DIR]</code>) or from a supported HuggingFace repo (e.g., <code>google/gemma-2-2b</code>).</li> </ul>"},{"location":"dev_cli/finetuned_model_export/#13-quantization-strategy","title":"1.3 Quantization Strategy","text":"<p>For LLMs targeting Ryzen AI, the recommended quantization strategy is:</p> <ul> <li>AWQ (Activation-aware Weight Quantization)</li> <li>Asymmetric quantization</li> <li>Activations: BF16/FP16/FP32</li> <li>Weights: UINT4</li> <li>Export: HuggingFace Transformers format </li> </ul>"},{"location":"dev_cli/finetuned_model_export/#14-example-quantization-commands","title":"1.4 Example Quantization Commands","text":"<pre><code>python quantize_quark.py --model_dir $MODEL_DIR \\\n    --output_dir &lt;path/to/output/model&gt; \\\n    --quant_scheme w_uint4_per_group_asym \\\n    --num_calib_data 128 \\\n    --quant_algo awq \\\n    --dataset pileval_for_awq_benchmark \\\n    --model_export hf_format \\\n    --group_size 128 \\\n    --group_size_per_layer lm_head 32 \\\n    --data_type float16 \\\n    --exclude_layers\n</code></pre> <p>Result: This will generate a quantized safetensors model in your output directory.</p>"},{"location":"dev_cli/finetuned_model_export/#step-2-export-to-onnx-for-ryzen-ai","title":"Step 2: Export to ONNX for Ryzen AI","text":"<p>Lemonade handles export and device specialization automatically. It detects model type and target device, exporting optimally for AMD hardware.</p> <p>Install support in Lemonade with:</p> <pre><code>pip install lemonade-sdk[dev,oga-ryzenai,model-generate] --extra-index-url https://pypi.amd.com/simple\n</code></pre>"},{"location":"dev_cli/finetuned_model_export/#usage","title":"Usage","text":"<pre><code>lemonade -i &lt;quantized_checkpoint&gt; oga-load --device &lt;hybrid|npu&gt; --dtype int4 [options]\n</code></pre> <p>Example using quantized model with prompt tool:</p> <pre><code>lemonade -i amd/Llama-3.2-1B-Instruct-awq-uint4-asym-g128-bf16-lmhead oga-load --device hybrid --dtype int4 llm-prompt -p \"Alice and Bob\" --max-new-tokens 10\n</code></pre> <p>Here, we have quantized the <code>Llama3.2-1B-Instruct</code> model using AWQ with a uint4 asymmetric quantization strategy and BF16 activations and uploaded it to Hugging Face. Now, using Lemonade, we\u2019re running inference with the built-in prompt tool on this fine-tuned, quantized model. Under the hood, Lemonade handles model export and optimization to ensure optimal inference.</p>"},{"location":"dev_cli/finetuned_model_export/#best-practices-notes","title":"Best Practices &amp; Notes","text":"<ul> <li>Model Compatibility: Ensure your model architecture is supported by Ryzen AI and Quark. Refer to the supported model list for details.</li> <li>Quantization Parameters: Adjust group size, data type, and quantization scheme as needed for your use case.</li> <li>Output Verification: After quantization and export, validate model performance on your target device. Check this notebook out to verify your export</li> <li>Documentation: Refer to Quark documentation and Lemonade's help (<code>lemonade --help</code>) for advanced options.</li> </ul>"},{"location":"dev_cli/finetuned_model_export/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you encounter errors during quantization or export, check that all dependencies are installed and your model checkpoint is accessible.</li> <li>For device-specific issues, consult the AMD forums or Ryzen AI documentation.</li> </ul>"},{"location":"server/","title":"Getting Started with Lemonade Server","text":"<p>\ud83c\udf4b Lemonade Server is a server interface that uses the standard Open AI API, allowing applications to integrate with local LLMs. This means that you can easily replace cloud-based LLMs with private and free LLMs that run locally on your own PC's NPU and GPU.</p> <p>Lemonade Server is available as a standalone tool with a one-click Windows GUI installer.</p> <p>Once you've installed, we recommend checking out these resources:</p> Documentation Description Supported Applications Explore applications that work out-of-the-box with Lemonade Server. Lemonade Server Concepts Background knowledge about local LLM servers and the OpenAI standard. <code>lemonade-server</code> CLI Guide Learn how to manage the server process and install new models using the command-line interface. Models List Browse a curated set of LLMs available for serving. Server Spec Review all supported OpenAI-compatible and Lemonade-specific API endpoints. Integration Guide Step-by-step instructions for integrating Lemonade Server into your own applications. <p>Note: if you want to develop Lemonade Server itself, you can install from source.</p>"},{"location":"server/#integrate-lemonade-server-with-your-application","title":"Integrate Lemonade Server with Your Application","text":"<p>Since Lemonade Server implements the standard OpenAI API specification, you can use any OpenAI-compatible client library by configuring it to use <code>http://localhost:8000/api/v1</code> as the base URL. A table containing official and popular OpenAI clients on different languages is shown below.</p> <p>Feel free to pick and choose your preferred language.</p> Python C++ Java C# Node.js Go Ruby Rust PHP openai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php"},{"location":"server/#python-client-example","title":"Python Client Example","text":"<pre><code>from openai import OpenAI\n\n# Initialize the client to use Lemonade Server\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\",\n    api_key=\"lemonade\"  # required but unused\n)\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",  # or any other available model\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\n# Print the response\nprint(completion.choices[0].message.content)\n</code></pre> <p>For more detailed integration instructions, see the Integration Guide.</p>"},{"location":"server/concepts/","title":"Local LLM Server Concepts","text":"<p>This document gives background information about the main concepts for local LLM servers and \ud83c\udf4bLemonade Server.</p> <p>The intention is to answer these FAQs:</p> <ul> <li>What is a Local Server?</li> <li>What is a Local LLM Server?</li> <li>What is the OpenAI Standard?</li> <li>How does the OpenAI Standard work?</li> </ul>"},{"location":"server/concepts/#what-is-a-local-server","title":"What is a Local Server?","text":"<p>First, let\u2019s clarify what we mean by <code>server software</code>, as it\u2019s sometimes confused with <code>server hardware</code>, which is the actual physical systems running in data centers. - <code>Server software</code> refers to a process running on a computer that listens for and responds to requests initiated by <code>client software</code> (i.e., applications). - <code>Server software</code> often runs on <code>server hardware</code>, but there are many examples of <code>server software</code> running on the same <code>client hardware</code> (laptop, desktop computer, tablet, or smartphone) as the <code>application</code>.</p> <p>A <code>local server</code> is <code>server software</code> that runs on <code>client hardware</code>.</p>"},{"location":"server/concepts/#what-is-a-local-llm-server","title":"What is a Local LLM Server?","text":"<p>Local LLM servers are an extremely popular way of deploying LLMs directly to <code>client hardware</code>. A few famous examples of local LLM servers include Ollama, llama-cpp-server, and Docker Model Runner.</p> <p>The local server process loads the LLM into memory and exposes it to client software for handling requests. Compared to integrating the LLM directly into the client software using C++ or Python APIs, this setup provides the following benefits:</p> Benefit Description Simplified integration C++/Python APIs are typically framework- (e.g., llama.cpp, OGA, etc.) and/or device- (e.g., CPU, GPU, NPU, etc.) specific. Local LLM servers, on the other hand, facilitate conversing with the LLM at a high level that abstracts these details away (see What is the OpenAI Standard?). Sharing LLMs between applications A single local LLM can take up a significant portion of system RAM. The local LLM server can share this LLM between multiple applications, rather than requiring each application to load its own LLM into RAM. Separation of concerns Installing and managing LLMs, enabling features like tool use and streaming generation, and building in fault tolerance can be tricky to implement. A local LLM server abstracts away this complexity, letting application developers stay focused on their app. Cloud-to-client development A common practice for LLM developers is to first develop their application using cloud LLMs, then switch to local LLMs later in development. Local and cloud LLM servers behave similarly from the application's perspective, which makes this transition seamless."},{"location":"server/concepts/#what-is-the-openai-standard","title":"What is the OpenAI Standard?","text":"<p>All LLM servers (cloud or local) adhere to an application-program interface (API). This API lets the <code>application</code> make LLM requests to the <code>server software</code>.</p> <p>While there are several popular LLM server APIs available in the LLM ecosystem, the OpenAI API has emerged as the industry standard because it is (at the time of this writing) the only API that meets these three criteria: 1. Dozens of popular LLM <code>servers</code> support OpenAI API. 1. Dozens of popular LLM <code>applications</code> support OpenAI API. 1. OpenAI API is broadly supported in both <code>local</code> and <code>cloud</code>.</p> <p>Crucially, while OpenAI offers their own LLM API-as-a-cloud-service, their API standard is rigorously documented and available for other cloud and local servers to adopt.</p>"},{"location":"server/concepts/#how-does-the-openai-standard-work","title":"How does the OpenAI Standard Work?","text":"<p>In the OpenAI API standard, applications and servers communicate in the form of a multi-role conversation. There are three \"roles\" in this context: the \"system\", the \"assistant\", and the \"user\".</p> Role Description System Allows the application to provide instructions to the LLM, such as defining its persona, what tools are available to it, what tasks it is supposed to help with or not help with, etc. Assistant Messages sent from the LLM to the application. User Messages sent from the application to the LLM. Often these messages are written by the application's end-user. <p>OpenAI also provides convenient libraries in JavaScript, Python, .Net, Java, and Go to help application and server developers adhere to the standard.</p> <p>For example, the following Python code demonstrates how an application can request an LLM response from the Lemonade Server:</p> <pre><code># Client library provided by OpenAI to automate request\n# and response processing with the server\nfrom openai import OpenAI\n\n# The base_url points to an LLM server, which can either be\n# local (localhost address) or cloud-based (web address)\nbase_url = f\"http://localhost:8000/api/v1\"\n\n# The `client` instance here provides APIs to request\n# LLM invocations from the server\nclient = OpenAI(\n    base_url=base_url,\n    api_key=\"lemonade\",  # required, but unused in Lemonade\n)\n\n# The `messages` list provides the history of messages from\n# the system, assistant, and user roles\nmessages = [\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\", \"content\":\"Hi, how are you?\"},\n]\n\n# This is the API call that sends the `messages` history to\n# the server's specific LLM `model`\n# It returns a `completion`, which is OpenAI's way of referring\n# to the LLM's reponse to the messages\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.1-8B-Instruct-Hybrid\",\n    messages=messages,\n)\n\n# This code gets the LLM's response from the `completion`\n# and prints it to the screen\nreponse = completion.choices[0].message.content\nprint(response)\n</code></pre> <p>The Python above will work with Lemonade Server, along with a variety of other cloud and local LLM servers, just by changing the <code>base_url</code>, <code>api_key</code>, and <code>model</code> as needed. This example demonstrates that details like deployment location (local vs. cloud), hardware type (GPU vs. NPU), and backend implementation (OGA vs. llama.cpp), etc. are hidden behind a unified interface.</p>"},{"location":"server/lemonade-server-cli/","title":"<code>lemonade-server</code> CLI","text":"<p>The <code>lemonade-server</code> command-line interface (CLI) provides a set of utility commands for managing the server. When you install Lemonade Server using the GUI installer, <code>lemonade-server</code> is added to your PATH so that it can be invoked from any terminal.</p> <p>Note: if you installed from source or PyPI, you should call <code>lemonade-server-dev</code> in your activated Python environment, instead of using <code>lemonade-server</code>.</p> <p><code>lemonade-server</code> provides these utilities:</p> Option/Command Description <code>-v</code>, <code>--version</code> Print the <code>lemonade-sdk</code> package version used to install Lemonade Server. <code>serve</code> Start the server process in the current terminal. See command options below. <code>status</code> Check if server is running. If it is, print the port number. <code>stop</code> Stop any running Lemonade Server process. <code>pull MODEL_NAME</code> Install an LLM named <code>MODEL_NAME</code>. See the server models guide for more information. <code>run MODEL_NAME</code> Start the server (if not already running) and chat with the specified model. Supports the same options as <code>serve</code>. <code>list</code> List all models. <p>Examples:</p> <pre><code># Start server with custom settings\nlemonade-server serve --port 8080 --log-level debug --llamacpp vulkan\n\n# Run a specific model with custom server settings\nlemonade-server run llama-3.2-3b-instruct --port 8080 --log-level debug --llamacpp rocm\n</code></pre>"},{"location":"server/lemonade-server-cli/#command-line-options-for-serve-and-run","title":"Command Line Options for <code>serve</code> and <code>run</code>","text":"<p>When using the <code>serve</code> command, you can configure the server with these additional options. The <code>run</code> command supports the same options but also requires a <code>MODEL_NAME</code> parameter:</p> <pre><code>lemonade-server serve [options]\nlemonade-server run MODEL_NAME [options]\n</code></pre> Option Description Default <code>--port [port]</code> Specify the port number to run the server on 8000 <code>--host [host]</code> Specify the host address for where to listen connections <code>localhost</code> <code>--log-level [level]</code> Set the logging level info <code>--llamacpp [vulkan\\|rocm]</code> Specify the LlamaCpp backend to use vulkan <code>--ctx-size [size]</code> Set the context size for the model. For llamacpp recipes, this sets the <code>--ctx-size</code> parameter for the llama server. For other recipes, prompts exceeding this size will be truncated. 4096 <p>These settings can also be provided via environment variables that Lemonade Server recognizes regardless of launch method: <code>LEMONADE_HOST</code>, <code>LEMONADE_PORT</code>, <code>LEMONADE_LOG_LEVEL</code>, <code>LEMONADE_LLAMACPP</code>, and <code>LEMONADE_CTX_SIZE</code>.</p> <p>The Lemonade Server integration guide provides more information about how these commands can be used to integrate Lemonade Server into an application.</p>"},{"location":"server/server_integration/","title":"Integrating with Lemonade Server","text":"<p>This guide provides instructions on how to integrate Lemonade Server into your application.</p> <p>There are two main ways in which Lemonade Sever might integrate into apps:</p> <ul> <li>User-Managed Server: User is responsible for installing and managing Lemonade Server.</li> <li>App-Managed Server: App is responsible for installing and managing Lemonade Server on behalf of the user.</li> </ul> <p>The first part of this guide contains instructions that are common for both integration approaches. The second part provides advanced instructions only needed for app-managed server integrations.</p>"},{"location":"server/server_integration/#general-instructions","title":"General Instructions","text":""},{"location":"server/server_integration/#identifying-existing-installation","title":"Identifying Existing Installation","text":"<p>To identify if Lemonade Server is installed on a system, you can use the <code>lemonade-server</code> CLI command, which is added to path when using our installer. This is a reliable method to:</p> <ul> <li>Verify if the server is installed.</li> <li>Check which version is currently available is running the command below.</li> </ul> <pre><code>lemonade-server --version\n</code></pre> <p>Note: The <code>lemonade-server</code> CLI command is added to PATH when using the Windows Installer (Lemonade_Server_Installer.exe). For Linux users or Python development environments, the command <code>lemonade-server-dev</code> is available when installing via pip.</p>"},{"location":"server/server_integration/#checking-server-status","title":"Checking Server Status","text":"<p>To identify whether or not the server is running anywhere on the system you may use the <code>status</code> command of <code>lemonade-server</code>.</p> <pre><code>lemonade-server status\n</code></pre> <p>This command will return either <code>Server is not running</code> or <code>Server is running on port &lt;PORT&gt;</code>.</p>"},{"location":"server/server_integration/#identifying-compatible-devices","title":"Identifying Compatible Devices","text":"<p>AMD Ryzen\u2122 AI <code>Hybrid</code> models are available on Windows 11 on all AMD Ryzen\u2122 AI 300 Series Processors. To programmatically identify supported devices, we recommend using a regular expression that checks if the CPU name converted to lowercase contains \"ryzen ai\" and a 3-digit number starting with 3 as shown below.</p> <pre><code>ryzen ai.*\\b3\\d{2}\\b\n</code></pre> <p>Explanation:</p> <ul> <li><code>ryzen ai</code>: Matches the literal phrase \"Ryzen AI\".</li> <li><code>.*</code>: Allows any characters (including spaces) to appear after \"Ryzen AI\".</li> <li><code>\\b3\\d{2}\\b</code>: Matches a three-digit number starting with 3, ensuring it's a standalone number.</li> </ul> <p>There are several ways to check the CPU name on a Windows computer. A reliable way of doing so is through cmd's <code>reg query</code> command as shown below.</p> <pre><code>reg query \"HKEY_LOCAL_MACHINE\\HARDWARE\\DESCRIPTION\\System\\CentralProcessor\\0\" /v ProcessorNameString\n</code></pre> <p>Once you capture the CPU name, make sure to convert it to lowercase before using the regular expression.</p>"},{"location":"server/server_integration/#downloading-server-installer","title":"Downloading Server Installer","text":"<p>The recommended way of directing users to the server installer is pointing users to our releases page at <code>https://github.com/lemonade-sdk/lemonade/releases</code>. Alternatively, you may also provide the direct path to the installer itself or download the installer programmatically as shown below:</p> <p>Latest version:</p> <pre><code>https://github.com/lemonade-sdk/lemonade/releases/latest/download/Lemonade_Server_Installer.exe\n</code></pre> <p>Specific version:</p> <pre><code>https://github.com/lemonade-sdk/lemonade/releases/download/v6.0.0/Lemonade_Server_Installer.exe\n</code></pre> <p>Please note that the Server Installer is only available on Windows. Apps that integrate with our server on a Linux machine must install Lemonade from source as described here.</p>"},{"location":"server/server_integration/#installing-additional-models","title":"Installing Additional Models","text":"<p>Lemonade Server installations always come with at least one LLM installed. If you want to install additional models on behalf of your users, the following tools are available:</p> <ul> <li>Discovering which LLMs are available:</li> <li>A human-readable list of supported models.</li> <li>A JSON file with the list of supported models is included in every Lemonade Server installation.</li> <li>Installing LLMs:</li> <li>The <code>pull</code> endpoint in the server.</li> <li><code>lemonade-server pull MODEL</code> on the command line interface.</li> </ul>"},{"location":"server/server_integration/#stand-alone-server-integration","title":"Stand-Alone Server Integration","text":"<p>Some apps might prefer to be responsible for installing and managing Lemonade Server on behalf of the user. This part of the guide includes steps for installing and running Lemonade Server so that your users don't have to install Lemonade Server separately.</p> <p>Definitions:</p> <ul> <li>Command line usage allows the server process to be launched programmatically, so that your application can manage starting and stopping the server process on your user's behalf.</li> <li>\"Silent installation\" refers to an automatic command for installing Lemonade Server without running any GUI or prompting the user for any questions. It does assume that the end-user fully accepts the license terms, so be sure that your own application makes this clear to the user.</li> </ul>"},{"location":"server/server_integration/#command-line-invocation","title":"Command Line Invocation","text":"<p>This command line invocation starts the Lemonade Server process so that your application can connect to it via REST API endpoints. To start the server, simply run the command below.</p> <pre><code>lemonade-server serve\n</code></pre> <p>By default, the server runs on port 8000. Optionally, you can specify a custom port using the --port argument:</p> <pre><code>lemonade-server serve --port 8123\n</code></pre> <p>You can also prevent the server from showing a system tray icon by using the <code>--no-tray</code> flag:</p> <pre><code>lemonade-server serve --no-tray\n</code></pre> <p>You can also run the server as a background process using a subprocess or any preferred method.</p> <p>To stop the server, you may use the <code>lemonade-server stop</code> command, or simply terminate the process you created by keeping track of its PID. Please do not run the <code>lemonade-server stop</code> command if your application has not started the server, as the server may be used by other applications.</p>"},{"location":"server/server_integration/#silent-installation","title":"Silent Installation","text":"<p>Silent installation runs <code>Lemonade_Server_Installer.exe</code> without a GUI and automatically accepts all prompts.</p> <p>In a <code>cmd.exe</code> terminal:</p> <p>Install with Ryzen AI hybrid support: </p> <pre><code>Lemonade_Server_Installer.exe /S /Extras=hybrid\n</code></pre> <p>Install without Ryzen AI hybrid support:</p> <pre><code>Lemonade_Server_Installer.exe /S\n</code></pre> <p>The install directory can also be changed from the default by using <code>/D</code> as the last argument. </p> <p>For example: </p> <pre><code>Lemonade_Server_Installer.exe /S /Extras=hybrid /D=C:\\a\\new\\path\n</code></pre> <p>Only <code>Qwen2.5-0.5B-Instruct-CPU</code> is installed by default in silent mode. If you wish to select additional models to download in silent mode, you may use the <code>/Models</code> argument.</p> <pre><code>Lemonade_Server_Installer.exe /S /Extras=hybrid /Models=\"Qwen2.5-0.5B-Instruct-CPU Llama-3.2-1B-Instruct-Hybrid\"\n</code></pre> <p>The available modes are documented here.</p> <p>Finally, if you don't want to create a desktop shortcut during installation, use the <code>/NoDesktopShortcut</code> parameter:</p> <pre><code>Lemonade_Server_Installer.exe /S /NoDesktopShortcut\n</code></pre>"},{"location":"server/server_models/","title":"\ud83c\udf4b Lemonade Server Models","text":"<p>This document provides the models we recommend for use with Lemonade Server.</p> <p>Click on any model to learn more details about it, such as the Lemonade Recipe used to load the model. Content:</p> <ul> <li>Model Management GUI</li> <li>Supported Models</li> <li>Naming Convention</li> <li>Model Storage and Management</li> <li>Installing Additional Models</li> </ul>"},{"location":"server/server_models/#model-management-gui","title":"Model Management GUI","text":"<p>Lemonade Server offers a model management GUI to help you see which models are available, install new models, and delete models. You can access this GUI by starting Lemonade Server, opening http://localhost:8000 in your web browser, and clicking the Model Management tab.</p>"},{"location":"server/server_models/#supported-models","title":"Supported Models","text":""},{"location":"server/server_models/#hot-models","title":"\ud83d\udd25 Hot Models","text":"Qwen3-30B-A3B-Instruct-2507-GGUF <pre><code>lemonade-server pull Qwen3-30B-A3B-Instruct-2507-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-30B-A3B-Instruct-2507-GGUF GGUF VariantQwen3-30B-A3B-Instruct-2507-Q4_0.gguf Recipellamacpp Labelshot Qwen3-Coder-30B-A3B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-Coder-30B-A3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF GGUF VariantQwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf Recipellamacpp Labelscoding, hot Cogito-v2-llama-109B-MoE-GGUF <pre><code>lemonade-server pull Cogito-v2-llama-109B-MoE-GGUF\n</code></pre> KeyValue Checkpointunsloth/cogito-v2-preview-llama-109B-MoE-GGUF GGUF VariantQ4_K_M Mmprojmmproj-F16.gguf Recipellamacpp Labelsvision, hot gpt-oss-120b-GGUF <pre><code>lemonade-server pull gpt-oss-120b-GGUF\n</code></pre> KeyValue Checkpointunsloth/gpt-oss-120b-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelshot, reasoning gpt-oss-20b-GGUF <pre><code>lemonade-server pull gpt-oss-20b-GGUF\n</code></pre> KeyValue Checkpointunsloth/gpt-oss-20b-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelshot, reasoning"},{"location":"server/server_models/#gguf","title":"GGUF","text":"Qwen3-0.6B-GGUF <pre><code>lemonade-server pull Qwen3-0.6B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-0.6B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-1.7B-GGUF <pre><code>lemonade-server pull Qwen3-1.7B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-1.7B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-4B-GGUF <pre><code>lemonade-server pull Qwen3-4B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-4B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-8B-GGUF <pre><code>lemonade-server pull Qwen3-8B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-8B-GGUF GGUF VariantQ4_1 Recipellamacpp Labelsreasoning DeepSeek-Qwen3-8B-GGUF <pre><code>lemonade-server pull DeepSeek-Qwen3-8B-GGUF\n</code></pre> KeyValue Checkpointunsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF GGUF VariantQ4_1 Recipellamacpp Labelsreasoning Qwen3-14B-GGUF <pre><code>lemonade-server pull Qwen3-14B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-14B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-30B-A3B-GGUF <pre><code>lemonade-server pull Qwen3-30B-A3B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-30B-A3B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-30B-A3B-Instruct-2507-GGUF <pre><code>lemonade-server pull Qwen3-30B-A3B-Instruct-2507-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-30B-A3B-Instruct-2507-GGUF GGUF VariantQwen3-30B-A3B-Instruct-2507-Q4_0.gguf Recipellamacpp Labelshot Qwen3-Coder-30B-A3B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-Coder-30B-A3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF GGUF VariantQwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf Recipellamacpp Labelscoding, hot Gemma-3-4b-it-GGUF <pre><code>lemonade-server pull Gemma-3-4b-it-GGUF\n</code></pre> KeyValue Checkpointggml-org/gemma-3-4b-it-GGUF GGUF VariantQ4_K_M Mmprojmmproj-model-f16.gguf Recipellamacpp Labelsvision Qwen2.5-VL-7B-Instruct-GGUF <pre><code>lemonade-server pull Qwen2.5-VL-7B-Instruct-GGUF\n</code></pre> KeyValue Checkpointggml-org/Qwen2.5-VL-7B-Instruct-GGUF GGUF VariantQ4_K_M Mmprojmmproj-Qwen2.5-VL-7B-Instruct-f16.gguf Recipellamacpp Labelsvision Llama-4-Scout-17B-16E-Instruct-GGUF <pre><code>lemonade-server pull Llama-4-Scout-17B-16E-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Llama-4-Scout-17B-16E-Instruct-GGUF GGUF VariantQ4_K_S Mmprojmmproj-F16.gguf Recipellamacpp Labelsvision Cogito-v2-llama-109B-MoE-GGUF <pre><code>lemonade-server pull Cogito-v2-llama-109B-MoE-GGUF\n</code></pre> KeyValue Checkpointunsloth/cogito-v2-preview-llama-109B-MoE-GGUF GGUF VariantQ4_K_M Mmprojmmproj-F16.gguf Recipellamacpp Labelsvision, hot nomic-embed-text-v1-GGUF <pre><code>lemonade-server pull nomic-embed-text-v1-GGUF\n</code></pre> KeyValue Checkpointnomic-ai/nomic-embed-text-v1-GGUF GGUF VariantQ4_K_S Recipellamacpp Labelsembeddings nomic-embed-text-v2-moe-GGUF <pre><code>lemonade-server pull nomic-embed-text-v2-moe-GGUF\n</code></pre> KeyValue Checkpointnomic-ai/nomic-embed-text-v2-moe-GGUF GGUF VariantQ8_0 Recipellamacpp Labelsembeddings bge-reranker-v2-m3-GGUF <pre><code>lemonade-server pull bge-reranker-v2-m3-GGUF\n</code></pre> KeyValue Checkpointpqnet/bge-reranker-v2-m3-Q8_0-GGUF Recipellamacpp Labelsreranking Devstral-Small-2507-GGUF <pre><code>lemonade-server pull Devstral-Small-2507-GGUF\n</code></pre> KeyValue Checkpointmistralai/Devstral-Small-2507_gguf GGUF VariantQ4_K_M Recipellamacpp Labelscoding Qwen2.5-Coder-32B-Instruct-GGUF <pre><code>lemonade-server pull Qwen2.5-Coder-32B-Instruct-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen2.5-Coder-32B-Instruct-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelsreasoning, coding gpt-oss-120b-GGUF <pre><code>lemonade-server pull gpt-oss-120b-GGUF\n</code></pre> KeyValue Checkpointunsloth/gpt-oss-120b-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelshot, reasoning gpt-oss-20b-GGUF <pre><code>lemonade-server pull gpt-oss-20b-GGUF\n</code></pre> KeyValue Checkpointunsloth/gpt-oss-20b-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelshot, reasoning"},{"location":"server/server_models/#hybrid","title":"Hybrid","text":"Llama-3.2-1B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.2-1B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Llama-3.2-3B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.2-3B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.2-3B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Phi-3-Mini-Instruct-Hybrid <pre><code>lemonade-server pull Phi-3-Mini-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Phi-3-mini-4k-instruct-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Qwen-1.5-7B-Chat-Hybrid <pre><code>lemonade-server pull Qwen-1.5-7B-Chat-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen1.5-7B-Chat-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Qwen-2.5-7B-Instruct-Hybrid <pre><code>lemonade-server pull Qwen-2.5-7B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen2.5-7B-Instruct-awq-uint4-asym-g128-lmhead-g32-fp16-onnx-hybrid Recipeoga-hybrid Qwen-2.5-3B-Instruct-Hybrid <pre><code>lemonade-server pull Qwen-2.5-3B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen2.5-3B-Instruct-awq-uint4-asym-g128-lmhead-g32-fp16-onnx-hybrid Recipeoga-hybrid Qwen-2.5-1.5B-Instruct-Hybrid <pre><code>lemonade-server pull Qwen-2.5-1.5B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen2.5-1.5B-Instruct-awq-uint4-asym-g128-lmhead-g32-fp16-onnx-hybrid Recipeoga-hybrid DeepSeek-R1-Distill-Llama-8B-Hybrid <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-Hybrid\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-hybrid Recipeoga-hybrid Labelsreasoning Mistral-7B-v0.3-Instruct-Hybrid <pre><code>lemonade-server pull Mistral-7B-v0.3-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Mistral-7B-Instruct-v0.3-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Llama-3.1-8B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.1-8B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.1-8B-Instruct-awq-asym-uint4-g128-lmhead-onnx-hybrid Recipeoga-hybrid Llama-xLAM-2-8b-fc-r-Hybrid <pre><code>lemonade-server pull Llama-xLAM-2-8b-fc-r-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid Recipeoga-hybrid"},{"location":"server/server_models/#npu","title":"NPU","text":"Qwen-2.5-7B-Instruct-NPU <pre><code>lemonade-server pull Qwen-2.5-7B-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-7B-Instruct-awq-g128-int4-asym-bf16-onnx-ryzen-strix Recipeoga-npu Qwen-2.5-1.5B-Instruct-NPU <pre><code>lemonade-server pull Qwen-2.5-1.5B-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-1.5B-Instruct-awq-g128-int4-asym-bf16-onnx-ryzen-strix Recipeoga-npu DeepSeek-R1-Distill-Llama-8B-NPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-NPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-g128-int4-asym-bf16-onnx-ryzen-strix Recipeoga-npu Mistral-7B-v0.3-Instruct-NPU <pre><code>lemonade-server pull Mistral-7B-v0.3-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Mistral-7B-Instruct-v0.3-awq-g128-int4-asym-bf16-onnx-ryzen-strix Recipeoga-npu Phi-3.5-Mini-Instruct-NPU <pre><code>lemonade-server pull Phi-3.5-Mini-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Phi-3.5-mini-instruct-awq-g128-int4-asym-bf16-onnx-ryzen-strix Recipeoga-npu"},{"location":"server/server_models/#cpu","title":"CPU","text":"Qwen2.5-0.5B-Instruct-CPU <pre><code>lemonade-server pull Qwen2.5-0.5B-Instruct-CPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-0.5B-Instruct-quantized_int4-float16-cpu-onnx Recipeoga-cpu Phi-3-Mini-Instruct-CPU <pre><code>lemonade-server pull Phi-3-Mini-Instruct-CPU\n</code></pre> KeyValue Checkpointamd/Phi-3-mini-4k-instruct_int4_float16_onnx_cpu Recipeoga-cpu Qwen-1.5-7B-Chat-CPU <pre><code>lemonade-server pull Qwen-1.5-7B-Chat-CPU\n</code></pre> KeyValue Checkpointamd/Qwen1.5-7B-Chat_uint4_asym_g128_float16_onnx_cpu Recipeoga-cpu DeepSeek-R1-Distill-Llama-8B-CPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-CPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-cpu Recipeoga-cpu Labelsreasoning DeepSeek-R1-Distill-Qwen-7B-CPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Qwen-7B-CPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-cpu Recipeoga-cpu Labelsreasoning"},{"location":"server/server_models/#naming-convention","title":"Naming Convention","text":"<p>The format of each Lemonade name is a combination of the name in the base checkpoint and the backend where the model will run. So, if the base checkpoint is <code>meta-llama/Llama-3.2-1B-Instruct</code>, and it has been optimized to run on Hybrid, the resulting name is <code>Llama-3.2-3B-Instruct-Hybrid</code>.</p>"},{"location":"server/server_models/#model-storage-and-management","title":"Model Storage and Management","text":"<p>Lemonade Server relies on Hugging Face Hub to manage downloading and storing models on your system. By default, Hugging Face Hub downloads models to <code>C:\\Users\\YOUR_USERNAME\\.cache\\huggingface\\hub</code>.</p> <p>For example, the Lemonade Server <code>Llama-3.2-3B-Instruct-Hybrid</code> model will end up at <code>C:\\Users\\YOUR_USERNAME\\.cache\\huggingface\\hub\\models--amd--Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid</code>. If you want to uninstall that model, simply delete that folder.</p> <p>You can change the directory for Hugging Face Hub by setting the <code>HF_HOME</code> or <code>HF_HUB_CACHE</code> environment variables.</p>"},{"location":"server/server_models/#installing-additional-models","title":"Installing Additional Models","text":"<p>Once you've installed Lemonade Server, you can install any model on this list using the <code>pull</code> command in the <code>lemonade-server</code> CLI.</p> <p>Example:</p> <pre><code>lemonade-server pull Qwen2.5-0.5B-Instruct-CPU\n</code></pre> <p>Note: <code>lemonade-server</code> is a utility that is added to your PATH when you install Lemonade Server with the GUI installer. If you are using Lemonade Server from a Python environment, use the <code>lemonade-server-dev pull</code> command instead.</p>"},{"location":"server/server_spec/","title":"Lemonade Server Spec","text":"<p>The <code>lemonade</code> SDK provides a standards-compliant server process that provides a REST API to enable communication with other applications.</p> <p>Lemonade Server currently supports two backends:</p> Backend Model Format Description ONNX Runtime GenAI (OGA) <code>.ONNX</code> Lemonade's built-in server, recommended for standard use on AMD platforms. Llama.cpp (Experimental) <code>.GGUF</code> Uses llama.cpp's Vulkan-powered llama-server backend. More details here."},{"location":"server/server_spec/#oga-endpoints-overview","title":"OGA Endpoints Overview","text":"<p>Right now, the key endpoints of the OpenAI API are available.</p> <p>We are also actively investigating and developing additional endpoints that will improve the experience of local applications.</p>"},{"location":"server/server_spec/#openai-compatible-endpoints","title":"OpenAI-Compatible Endpoints","text":"<ul> <li>POST <code>/api/v1/chat/completions</code> - Chat Completions (messages -&gt; completion)</li> <li>POST <code>/api/v1/completions</code> - Text Completions (prompt -&gt; completion)</li> <li>POST <code>api/v1/responses</code> - Chat Completions (prompt|messages -&gt; event)</li> <li>GET <code>/api/v1/models</code> - List models available locally</li> </ul>"},{"location":"server/server_spec/#additional-endpoints","title":"Additional Endpoints","text":"<p>\ud83d\udea7 These additional endpoints are a preview that is under active development. The API specification is subject to change.</p> <p>These additional endpoints were inspired by the LM Studio REST API, Ollama API, and OpenAI API.</p> <p>They focus on enabling client applications by extending existing cloud-focused APIs (e.g., OpenAI) to also include the ability to load and unload models before completion requests are made. These extensions allow for a greater degree of UI/UX responsiveness in native applications by allowing applications to:</p> <ul> <li>Pre-load models at UI-loading-time, as opposed to completion-request time.</li> <li>Load models from the local system that were downloaded by other applications (i.e., a common system-wide models cache).</li> <li>Unload models to save memory space.</li> </ul> <p>The additional endpoints under development are:</p> <ul> <li>POST <code>/api/v1/pull</code> - Install a model</li> <li>POST <code>/api/v1/load</code> - Load a model</li> <li>POST <code>/api/v1/unload</code> - Unload a model</li> <li>POST <code>/api/v1/params</code> - Set generation parameters</li> <li>GET <code>/api/v1/health</code> - Check server health</li> <li>GET <code>/api/v1/stats</code> - Performance statistics from the last request</li> <li>GET <code>/api/v1/system-info</code> - System information and device enumeration</li> </ul> <p>\ud83d\udea7 We are in the process of developing this interface. Let us know what's important to you on Github or by email (lemonade at amd dot com).</p>"},{"location":"server/server_spec/#start-the-rest-api-server","title":"Start the REST API Server","text":"<p>NOTE: This server is intended for use on local systems only. Do not expose the server port to the open internet.</p>"},{"location":"server/server_spec/#windows-installer","title":"Windows Installer","text":"<p>See the Lemonade Server getting started instructions. </p>"},{"location":"server/server_spec/#python-environment","title":"Python Environment","text":"<p>If you have Lemonade installed in a Python environment, simply activate it and run the following command to start the server:</p> <pre><code>lemonade-server-dev serve\n</code></pre>"},{"location":"server/server_spec/#openai-compatible-endpoints_1","title":"OpenAI-Compatible Endpoints","text":""},{"location":"server/server_spec/#post-apiv1chatcompletions","title":"<code>POST /api/v1/chat/completions</code>","text":"<p>Chat Completions API. You provide a list of messages and receive a completion. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters","title":"Parameters","text":"Parameter Required Description Status <code>messages</code> Yes Array of messages in the conversation. Each message should have a <code>role</code> (\"user\" or \"assistant\") and <code>content</code> (the message text). <code>model</code> Yes The model to use for the completion. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <code>stop</code> No Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Can be a string or an array of strings. <code>logprobs</code> No Include log probabilities of the output tokens. If true, returns the log probability of each output token. Defaults to false. <code>temperature</code> No What sampling temperature to use. <code>repeat_penalty</code> No Number between 1.0 and 2.0. 1.0 means no penalty. Higher values discourage repetition. <code>top_k</code> No Integer that controls the number of top tokens to consider during sampling. <code>top_p</code> No Float between 0.0 and 1.0 that controls the cumulative probability of top tokens to consider during nucleus sampling. <code>tools</code> No A list of tools the model may call. <code>max_tokens</code> No An upper bound for the number of tokens that can be generated for a completion. Mutually exclusive with <code>max_completion_tokens</code>. This value is now deprecated by OpenAI in favor of <code>max_completion_tokens</code> <code>max_completion_tokens</code> No An upper bound for the number of tokens that can be generated for a completion. Mutually exclusive with <code>max_tokens</code>. <p>Note: The value for <code>model</code> is either a Lemonade Server model name, or a checkpoint that has been pre-loaded using the load endpoint.</p>"},{"location":"server/server_spec/#example-request","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest `\n  -Uri \"http://localhost:8000/api/v1/chat/completions\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the population of Paris?\"\n      }\n    ],\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"messages\": [\n          {\"role\": \"user\", \"content\": \"What is the population of Paris?\"}\n        ],\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format","title":"Response format","text":"Non-streaming responsesStreaming responses <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"chat.completion\",\n  \"created\": 1742927481,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Paris has a population of approximately 2.2 million people in the city proper.\"\n    },\n    \"finish_reason\": \"stop\"\n  }]\n}\n</code></pre> <p>For streaming responses, the API returns a stream of server-sent events (however, Open AI recommends using their streaming libraries for parsing streaming responses):</p> <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1742927481,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"choices\": [{\n    \"index\": 0,\n    \"delta\": {\n      \"role\": \"assistant\",\n      \"content\": \"Paris\"\n    }\n  }]\n}\n</code></pre>"},{"location":"server/server_spec/#post-apiv1completions","title":"<code>POST /api/v1/completions</code>","text":"<p>Text Completions API. You provide a prompt and receive a completion. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters_1","title":"Parameters","text":"Parameter Required Description Status <code>prompt</code> Yes The prompt to use for the completion. <code>model</code> Yes The model to use for the completion. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <code>stop</code> No Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Can be a string or an array of strings. <code>echo</code> No Echo back the prompt in addition to the completion. Available on non-streaming mode. <code>logprobs</code> No Include log probabilities of the output tokens. If true, returns the log probability of each output token. Defaults to false. Only available when <code>stream=False</code>. <code>temperature</code> No What sampling temperature to use. <code>repeat_penalty</code> No Number between 1.0 and 2.0. 1.0 means no penalty. Higher values discourage repetition. <code>top_k</code> No Integer that controls the number of top tokens to consider during sampling. <code>top_p</code> No Float between 0.0 and 1.0 that controls the cumulative probability of top tokens to consider during nucleus sampling. <code>max_tokens</code> No An upper bound for the number of tokens that can be generated for a completion, including input tokens. <p>Note: The value for <code>model</code> is either a Lemonade Server model name, or a checkpoint that has been pre-loaded using the load endpoint.</p>"},{"location":"server/server_spec/#example-request_1","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest -Uri \"http://localhost:8000/api/v1/completions\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"prompt\": \"What is the population of Paris?\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"prompt\": \"What is the population of Paris?\",\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_1","title":"Response format","text":"<p>The following format is used for both streaming and non-streaming responses:</p> <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"text_completion\",\n  \"created\": 1742927481,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"choices\": [{\n    \"index\": 0,\n    \"text\": \"Paris has a population of approximately 2.2 million people in the city proper.\",\n    \"finish_reason\": \"stop\"\n  }],\n}\n</code></pre>"},{"location":"server/server_spec/#post-apiv1responses","title":"<code>POST /api/v1/responses</code>","text":"<p>Responses API. You provide an input and receive a response. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters_2","title":"Parameters","text":"Parameter Required Description Status <code>input</code> Yes A list of dictionaries or a string input for the model to respond to. <code>model</code> Yes The model to use for the response. <code>max_output_tokens</code> No The maximum number of output tokens to generate. <code>temperature</code> No What sampling temperature to use. <code>repeat_penalty</code> No Number between 1.0 and 2.0. 1.0 means no penalty. Higher values discourage repetition. <code>top_k</code> No Integer that controls the number of top tokens to consider during sampling. <code>top_p</code> No Float between 0.0 and 1.0 that controls the cumulative probability of top tokens to consider during nucleus sampling. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <p>Note: The value for <code>model</code> is either a Lemonade Server model name, or a checkpoint that has been pre-loaded using the load endpoint.</p>"},{"location":"server/server_spec/#streaming-events","title":"Streaming Events","text":"<p>The Responses API uses semantic events for streaming. Each event is typed with a predefined schema, so you can listen for events you care about. Our initial implementation only offers support to:</p> <ul> <li><code>response.created</code></li> <li><code>response.output_text.delta</code></li> <li><code>response.completed</code></li> </ul> <p>For a full list of event types, see the API reference for streaming.</p>"},{"location":"server/server_spec/#example-request_2","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest -Uri \"http://localhost:8000/api/v1/responses\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"input\": \"What is the population of Paris?\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"input\": \"What is the population of Paris?\",\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_2","title":"Response format","text":"Non-streaming responsesStreaming Responses <pre><code>{\n  \"id\": \"0\",\n  \"created_at\": 1746225832.0,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"object\": \"response\",\n  \"output\": [{\n    \"id\": \"0\",\n    \"content\": [{\n      \"annotations\": [],\n      \"text\": \"Paris has a population of approximately 2.2 million people in the city proper.\"\n    }]\n  }]\n}\n</code></pre> <p>For streaming responses, the API returns a series of events. Refer to OpenAI streaming guide for details.</p>"},{"location":"server/server_spec/#get-apiv1models","title":"<code>GET /api/v1/models</code>","text":"<p>Returns a list of key models available on the server in an OpenAI-compatible format. We also expanded each model object with the <code>checkpoint</code> and <code>recipe</code> fields, which may be used to load a model using the <code>load</code> endpoint.</p> <p>This list is curated based on what works best for Ryzen AI Hybrid. Only models available locally are shown.</p>"},{"location":"server/server_spec/#parameters_3","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_3","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/models\n</code></pre>"},{"location":"server/server_spec/#response-format_3","title":"Response format","text":"<pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"Qwen2.5-0.5B-Instruct-CPU\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"amd/Qwen2.5-0.5B-Instruct-quantized_int4-float16-cpu-onnx\",\n      \"recipe\": \"oga-cpu\"\n    },\n    {\n      \"id\": \"Llama-3.2-1B-Instruct-Hybrid\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid\",\n      \"recipe\": \"oga-hybrid\"\n    },\n  ]\n}\n</code></pre>"},{"location":"server/server_spec/#additional-endpoints_1","title":"Additional Endpoints","text":""},{"location":"server/server_spec/#get-apiv1pull","title":"<code>GET /api/v1/pull</code>","text":"<p>Register and install models for use with Lemonade Server.</p>"},{"location":"server/server_spec/#parameters_4","title":"Parameters","text":"<p>The Lemonade Server built-in model registry has a collection of model names that can be pulled and loaded. The <code>pull</code> endpoint can install any registered model, and it can also register-then-install any model available on Hugging Face.</p> <p>Install a Model that is Already Registered</p> Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to install. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Installed model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p> <p>Register and Install a Model</p> <p>Registration will place an entry for that model in the <code>user_models.json</code> file, which is located in the user's Lemonade cache (default: <code>~/.cache/lemonade</code>). Then, the model will be installed. Once the model is registered and installed, it will show up in the <code>models</code> endpoint alongside the built-in models and can be loaded.</p> <p>The <code>recipe</code> field defines which software framework and device will be used to load and run the model. For more information on OGA and Hugging Face recipes, see the Lemonade API README. For information on GGUF recipes, see llamacpp.</p> <p>Note: the <code>model_name</code> for registering a new model must use the <code>user</code> namespace, to prevent collisions with built-in models. For example, <code>user.Phi-4-Mini-GGUF</code>.</p> Parameter Required Description <code>model_name</code> Yes Namespaced Lemonade Server model name to register and install. <code>checkpoint</code> Yes HuggingFace checkpoint to install. <code>recipe</code> Yes Lemonade API recipe to load the model with. <code>reasoning</code> No Whether the model is a reasoning model, like DeepSeek (default: false). <code>mmproj</code> No Multimodal Projector (mmproj) file to use for vision models. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"user.Phi-4-Mini-GGUF\",\n    \"checkpoint\": \"unsloth/Phi-4-mini-instruct-GGUF:Q4_K_M\",\n    \"recipe\": \"llamacpp\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Installed model: user.Phi-4-Mini-GGUF\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1delete","title":"<code>POST /api/v1/delete</code>","text":"<p>Delete a model by removing it from local storage. If the model is currently loaded, it will be unloaded first.</p>"},{"location":"server/server_spec/#parameters_5","title":"Parameters","text":"Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to delete. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/delete \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Deleted model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p> <p></p>"},{"location":"server/server_spec/#get-apiv1load","title":"<code>GET /api/v1/load</code>","text":"<p>Explicitly load a registered model into memory. This is useful to ensure that the model is loaded before you make a request. Installs the model if necessary.</p>"},{"location":"server/server_spec/#parameters_6","title":"Parameters","text":"Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to load. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Loaded model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1unload","title":"<code>POST /api/v1/unload</code>","text":"<p>Explicitly unload a model from memory. This is useful to free up memory while still leaving the server process running (which takes minimal resources but a few seconds to start).</p>"},{"location":"server/server_spec/#parameters_7","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_4","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/unload\n</code></pre>"},{"location":"server/server_spec/#response-format_4","title":"Response format","text":"<p><pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Model unloaded successfully\"\n}\n</code></pre> In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1params","title":"<code>POST /api/v1/params</code>","text":"<p>Set the generation parameters for text completion. These parameters will persist across requests until changed.</p>"},{"location":"server/server_spec/#parameters_8","title":"Parameters","text":"Parameter Required Description <code>temperature</code> No Controls randomness in the output. Higher values (e.g. 0.8) make the output more random, lower values (e.g. 0.2) make it more focused and deterministic. Defaults to 0.7. <code>top_p</code> No Controls diversity via nucleus sampling. Keeps the cumulative probability of tokens above this value. Defaults to 0.95. <code>top_k</code> No Controls diversity by limiting to the k most likely next tokens. Defaults to 50. <code>min_length</code> No The minimum length of the generated text in tokens. Defaults to 0. <code>max_length</code> No The maximum length of the generated text in tokens. Defaults to 2048. <code>do_sample</code> No Whether to use sampling (true) or greedy decoding (false). Defaults to true."},{"location":"server/server_spec/#example-request_5","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/params \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"max_length\": 1000\n  }'\n</code></pre>"},{"location":"server/server_spec/#response-format_5","title":"Response format","text":"<p><pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Generation parameters set successfully\",\n  \"params\": {\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"min_length\": 0,\n    \"max_length\": 1000,\n    \"do_sample\": true\n  }\n}\n</code></pre> In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#get-apiv1health","title":"<code>GET /api/v1/health</code>","text":"<p>Check the health of the server. This endpoint will also return the currently loaded model.</p>"},{"location":"server/server_spec/#parameters_9","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_6","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/health\n</code></pre>"},{"location":"server/server_spec/#response-format_6","title":"Response format","text":"<pre><code>{\n  \"status\": \"ok\",\n  \"checkpoint_loaded\": \"amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid\",\n  \"model_loaded\": \"Llama-3.2-1B-Instruct-Hybrid\",\n}\n</code></pre>"},{"location":"server/server_spec/#get-apiv1stats","title":"<code>GET /api/v1/stats</code>","text":"<p>Performance statistics from the last request.</p>"},{"location":"server/server_spec/#parameters_10","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_7","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/stats\n</code></pre>"},{"location":"server/server_spec/#response-format_7","title":"Response format","text":"<pre><code>{\n  \"time_to_first_token\": 2.14,\n  \"tokens_per_second\": 33.33,\n  \"input_tokens\": 128,\n  \"output_tokens\": 5,\n  \"decode_token_times\": [0.01, 0.02, 0.03, 0.04, 0.05]\n}\n</code></pre>"},{"location":"server/server_spec/#get-apiv1system-info","title":"<code>GET /api/v1/system-info</code>","text":"<p>System information endpoint that provides complete hardware details and device enumeration.</p>"},{"location":"server/server_spec/#parameters_11","title":"Parameters","text":"Parameter Required Description Status <code>verbose</code> No Include detailed system information. When <code>false</code> (default), returns essential information (OS, processor, memory, devices). When <code>true</code>, includes additional details like Python packages and extended system information."},{"location":"server/server_spec/#example-request_8","title":"Example request","text":"Basic system informationDetailed system information <pre><code>curl \"http://localhost:8000/api/v1/system-info\"\n</code></pre> <pre><code>curl \"http://localhost:8000/api/v1/system-info?verbose=true\"\n</code></pre>"},{"location":"server/server_spec/#response-format_8","title":"Response format","text":"Basic response (verbose=false) <pre><code>{\n  \"OS Version\": \"Windows-10-10.0.26100-SP0\",\n  \"Processor\": \"AMD Ryzen AI 9 HX 375 w/ Radeon 890M\",\n  \"Physical Memory\": \"32.0 GB\",\n  \"devices\": {\n    \"cpu\": {\n      \"name\": \"AMD Ryzen AI 9 HX 375 w/ Radeon 890M\",\n      \"cores\": 12,\n      \"threads\": 24,\n      \"available\": true\n    },\n    \"amd_igpu\": {\n      \"name\": \"AMD Radeon(TM) 890M Graphics\",\n      \"memory_mb\": 512,\n      \"driver_version\": 32.0.12010.10001,\n      \"available\": true\n    },\n    \"amd_dgpu\": [],\n    \"npu\": {\n      \"name\": \"AMD NPU\",\n      \"driver_version\": \"32.0.203.257\",\n      \"power_mode\": \"Default\",\n      \"available\": true\n    }\n  }\n}\n</code></pre>"},{"location":"server/server_spec/#debugging","title":"Debugging","text":"<p>To help debug the Lemonade server, you can use the <code>--log-level</code> parameter to control the verbosity of logging information. The server supports multiple logging levels that provide increasing amounts of detail about server operations.</p> <pre><code>lemonade-server serve --log-level [level]\n</code></pre> <p>Where <code>[level]</code> can be one of:</p> <ul> <li>critical: Only critical errors that prevent server operation.</li> <li>error: Error conditions that might allow continued operation.</li> <li>warning: Warning conditions that should be addressed.</li> <li>info: (Default) General informational messages about server operation.</li> <li>debug: Detailed diagnostic information for troubleshooting, including metrics such as input/output token counts, Time To First Token (TTFT), and Tokens Per Second (TPS).</li> <li>trace: Very detailed tracing information, including everything from debug level plus all input prompts.</li> </ul>"},{"location":"server/server_spec/#experimental-gguf-support","title":"Experimental GGUF Support","text":"<p>The OGA models (<code>*-CPU</code>, <code>*-Hybrid</code>) available in Lemonade Server use Lemonade's built-in server implementation. However, Lemonade SDK v7.0.1 introduced experimental support for llama.cpp's Vulkan <code>llama-server</code> as an alternative backend for CPU and GPU.</p> <p>The <code>llama-server</code> backend works with Lemonade's suggested <code>*-GGUF</code> models, as well as any .gguf model from Hugging Face. Windows and Ubuntu Linux are supported. Details: - Lemonade Server wraps <code>llama-server</code> with support for the <code>lemonade-server</code> CLI, client web app, and endpoints (e.g., <code>models</code>, <code>pull</code>, <code>load</code>, etc.).   - The <code>chat/completions</code>, <code>embeddings</code>, and <code>reranking</code> endpoints are supported.    - Non-chat <code>completions</code>, and <code>responses</code> are not supported at this time. - A single Lemonade Server process can seamlessly switch between OGA and GGUF models.   - Lemonade Server will attempt to load models onto GPU with Vulkan first, and if that doesn't work it will fall back to CPU.   - From the end-user's perspective, OGA vs. GGUF should be completely transparent: they wont be aware of whether the built-in server or <code>llama-server</code> is serving their model.</p>"},{"location":"server/server_spec/#installing-gguf-models","title":"Installing GGUF Models","text":"<p>To install an arbitrary GGUF from Hugging Face, open the Lemonade web app by navigating to http://localhost:8000 in your web browser and click the Model Management tab.</p>"},{"location":"server/server_spec/#platform-support-matrix","title":"Platform Support Matrix","text":"Platform Vulkan GPU x64 CPU Windows \u2705 \u2705 Ubuntu \u2705 \u2705 Other Linux \u26a0\ufe0f* \u26a0\ufe0f* <p>*Other Linux distributions may work but are not officially supported.</p>"},{"location":"server/apps/","title":"Lemonade Server Examples","text":"<p>Many applications today utilize OpenAI models like ChatGPT through APIs such as:</p> <p><code>POST https://api.openai.com/v1/chat/completions</code></p> <p>This API call triggers the ChatGPT model to generate responses for a chat. With Lemonade Server, we are replacing the OpenAI endpoint with a local LLM. The new API call becomes:</p> <p><code>POST http://localhost:8000/api/v1/chat/completions</code></p> <p>This allows the same application to leverage local LLMs instead of relying on OpenAI's cloud-based models. The guides in this folder show how to connect Lemonade Server to popular applications to enable local LLM execution. To run these examples, you'll need a Windows PC.</p>"},{"location":"server/apps/#video-tutorials","title":"\ud83c\udfa5 Video Tutorials","text":"Links to the video tutorials available are provided in the third column of the following table.   App Guide Video Open WebUI How to chat with Lemonade LLMs in Open WebUI Watch Demo Continue.dev How to use Lemonade LLMs as a coding assistant in Continue Watch Demo Microsoft AI Toolkit Experimenting with Lemonade LLMs in VS Code using Microsoft's AI Toolkit Watch Demo GAIA An application for running LLMs locally, includes a ChatBot, YouTube Agent, and more Watch Demo Microsoft AI Dev Gallery Microsoft's showcase application for exploring AI capabilities coming soon CodeGPT How to use Lemonade LLMs as a coding assistant in CodeGPT coming soon MindCraft How to use Lemonade LLMs as a Minecraft agent coming soon wut Terminal assistant that uses Lemonade LLMs to explain errors coming soon AnythingLLM Running agents locally with Lemonade and AnythingLLM coming soon lm-eval A unified framework to test generative language models on a large number of different evaluation tasks. coming soon PEEL Using Local LLMs in Windows PowerShell coming soon"},{"location":"server/apps/#looking-for-installation-help","title":"\ud83d\udce6 Looking for Installation Help?","text":"<p>To set up Lemonade Server, check out the Lemonade Server guide for installation instructions and the server spec to learn more about the functionality. For more information about \ud83c\udf4b Lemonade SDK, see the Lemonade SDK README.</p>"},{"location":"server/apps/#support","title":"\ud83d\udee0\ufe0f Support","text":"<p>If you encounter any issues or have questions, feel free to:</p> <ul> <li>File an issue on our GitHub Issues page.</li> <li>Email us at lemonade@amd.com.</li> </ul>"},{"location":"server/apps/#want-to-add-an-example","title":"\ud83d\udca1 Want to Add an Example?","text":"<p>If you've connected Lemonade to a new application, feel free to contribute a guide by following our contribution guide found here or let us know at lemonade@amd.com.</p>"},{"location":"server/apps/ai-dev-gallery/","title":"AI Dev Gallery with Lemonade Server","text":""},{"location":"server/apps/ai-dev-gallery/#overview","title":"Overview","text":"<p>AI Dev Gallery is Microsoft's showcase application that demonstrates various AI capabilities through built-in samples and applications. It provides an easy way to explore and experiment with different AI models and scenarios, including text generation, chat applications, and more.</p> <p>AI Dev Gallery has native integration with Lemonade Server, which means it can automatically detect and connect to your local Lemonade instance without manual URL configuration.</p>"},{"location":"server/apps/ai-dev-gallery/#expectations","title":"Expectations","text":"<p>AI Dev Gallery works well with most models available in Lemonade. The built-in samples are designed to work with various model types and sizes, making it a great tool for testing and exploring different AI capabilities locally.</p> <p>The application provides a user-friendly interface for experimenting with AI models through pre-built scenarios, making it accessible for both beginners and advanced users.</p>"},{"location":"server/apps/ai-dev-gallery/#setup","title":"Setup","text":""},{"location":"server/apps/ai-dev-gallery/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Important: Make sure your Lemonade Server is running before opening AI Dev Gallery.</li> </ol>"},{"location":"server/apps/ai-dev-gallery/#install-ai-dev-gallery","title":"Install AI Dev Gallery","text":"<ol> <li>Open the Microsoft Store on Windows.</li> <li>Search for \"AI Dev Gallery\" by Microsoft Corporation.</li> <li>Click \"Install\" to download and install the application.</li> </ol> <p>Alternatively, you can access AI Dev Gallery directly through aka.ms/ai-dev-gallery.</p>"},{"location":"server/apps/ai-dev-gallery/#connect-to-lemonade","title":"Connect to Lemonade","text":"<p>AI Dev Gallery has native integration with Lemonade Server, so no manual configuration is required. The application will automatically detect your running Lemonade Server instance.</p> <p>Important: Ensure your Lemonade Server is running before launching AI Dev Gallery.</p>"},{"location":"server/apps/ai-dev-gallery/#usage","title":"Usage","text":"<p>AI Dev Gallery provides various built-in applications and samples to explore AI capabilities:</p>"},{"location":"server/apps/ai-dev-gallery/#quick-start","title":"Quick Start","text":"<ol> <li>Launch AI Dev Gallery.</li> <li>Navigate to Samples \u2192 Text \u2192 Chat (or another text/code sample).</li> <li>Click on the model selector above the chat window.</li> <li>Select Lemonade from the available providers.</li> <li>Choose your preferred model from the list of available models.</li> </ol>"},{"location":"server/apps/ai-dev-gallery/#supported-scenarios","title":"Supported Scenarios","text":"<p>AI Dev Gallery supports various AI scenarios through its sample applications with Lemonade integration:</p> <p>Text Processing:</p> <ul> <li>Conversational AI: Chat and Semantic Kernel Chat for interactive conversations</li> <li>Content Generation: Generate text for various purposes and creative writing</li> <li>Language Tasks: Translation, grammar checking, and paraphrasing</li> <li>Text Analysis: Sentiment analysis and content moderation</li> <li>Information Retrieval: Semantic search and retrieval augmented generation</li> <li>Text Enhancement: Summarization and custom parameter configurations</li> </ul> <p>Code Assistance:</p> <ul> <li>Code Generation: Create code snippets and programs</li> <li>Code Analysis: Explain existing code and understand functionality</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#tips-for-best-experience","title":"Tips for Best Experience","text":"<ul> <li>Start your Lemonade Server before opening AI Dev Gallery</li> <li>Try different models to see how they perform across various scenarios</li> <li>Explore different sample categories to understand various AI capabilities</li> <li>Use the built-in samples as starting points for your own AI experiments</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#troubleshooting","title":"Troubleshooting","text":""},{"location":"server/apps/ai-dev-gallery/#ai-dev-gallery-doesnt-detect-lemonade","title":"AI Dev Gallery doesn't detect Lemonade","text":"<ul> <li>Ensure Lemonade Server is running and accessible at <code>http://localhost:8000</code></li> <li>Restart AI Dev Gallery after ensuring Lemonade Server is running</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#models-not-appearing-in-the-selector","title":"Models not appearing in the selector","text":"<ul> <li>Open <code>http://localhost:8000</code> in a browser and make sure to download the models you want to use through the \"Model Manager\" tab.</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#additional-resources","title":"Additional Resources","text":"<ul> <li>AI Dev Gallery Website</li> <li>Lemonade Server Models</li> </ul>"},{"location":"server/apps/ai-toolkit/","title":"Microsoft AI Toolkit for VS Code","text":""},{"location":"server/apps/ai-toolkit/#overview","title":"Overview","text":"<p>The AI Toolkit for Visual Studio Code is a VS Code extension that simplifies generative AI app development by bringing together cutting-edge AI development tools and models from various catalogs. It supports running AI models locally or connecting to remote models via API keys.</p>"},{"location":"server/apps/ai-toolkit/#demo-video","title":"Demo Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p>"},{"location":"server/apps/ai-toolkit/#expectations","title":"Expectations","text":"<p>We have found that most LLMs work well with this application. </p> <p>However, the <code>Inference Parameters</code> option is not fully supported, as Lemonade Server currently does not accept those as inputs (see server_spec.md for details).</p>"},{"location":"server/apps/ai-toolkit/#setup","title":"Setup","text":""},{"location":"server/apps/ai-toolkit/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/ai-toolkit/#install-ai-toolkit-for-vs-code","title":"Install AI Toolkit for VS Code","text":"<ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search for \"AI Toolkit for Visual Studio Code\" in the Extensions Marketplace search bar.</li> <li>Select the AI Toolkit extension and click install.</li> </ol> <p>This will add an AI Toolkit icon to your VS Code Activity Bar.</p>"},{"location":"server/apps/ai-toolkit/#connect-lemonade-to-ai-toolkit","title":"Connect Lemonade to AI Toolkit","text":"<p>The AI Toolkit now supports \"Bring Your Own Model\" functionality, allowing you to connect to models served via the OpenAI API standard, which Lemonade uses.</p> <ol> <li>Open the AI Toolkit tab in your VS Code Activity Bar.</li> <li>In the right corner of the \"My Models\" section, click the \"+\" button to \"Add model for remote inference\".</li> <li>Select \"Add a custom model\".</li> <li>When prompted to \"Enter OpenAI chat completion endpoint URL\" enter:     <pre><code>http://localhost:8000/api/v1/chat/completions\n</code></pre></li> <li>When prompted to \"Enter the exact model name as in the API\" select a model (e.g., <code>Phi-3-Mini-Instruct-Hybrid</code>)<ul> <li>Note: You can get a list of all models available here.</li> </ul> </li> <li>Select the same name as the display model name.</li> <li>Skip the HTTP authentication step by pressing \"Enter\".</li> </ol>"},{"location":"server/apps/ai-toolkit/#usage","title":"Usage","text":"<p>Once you've set up the Lemonade model in AI Toolkit, you can:</p> <ol> <li>Use the AI Playground tool to directly interact with your added model.</li> <li>Use the Prompt Builder tool to craft effective prompts for your AI models.</li> <li>Use the Bulk Run tool to compute responses for custom datasets and easily visualize those responses on a table format.</li> <li>Use the Evaluation tool to quickly assess your model's coherence, fluency, relevance, and similarity, as well as to compute BLEU, F1, GLEU, and Meteor scores.</li> </ol>"},{"location":"server/apps/ai-toolkit/#additional-resources","title":"Additional Resources","text":"<ul> <li>AI Toolkit for VS Code Documentation</li> <li>AI Toolkit GitHub Repository</li> <li>Bring Your Own Models on AI Toolkit</li> </ul>"},{"location":"server/apps/anythingLLM/","title":"Running agents locally with Lemonade and AnythingLLM","text":""},{"location":"server/apps/anythingLLM/#overview","title":"Overview","text":"<p>AnythingLLM is a versatile local LLM platform that allows you to chat with your documents and code using a variety of models. It supports the OpenAI-compatible API interface, allowing easy integration with local servers like Lemonade.</p> <p>This guide will help you configure AnythingLLM to use Lemonade's OpenAI-compatible server, and utilize the powerful <code>@agent</code> capability to interact with documents, webpages, and more.</p>"},{"location":"server/apps/anythingLLM/#expectations","title":"Expectations","text":"<p>Lemonade integrates best with AnythingLLM when using models such as <code>Qwen-1.5-7B-Chat-Hybrid</code> and <code>Llama-3.2-1B-Instruct-Hybrid</code>, both of which support a context length of up to 3,000 tokens.</p> <p>Keep in mind that when using the <code>@agent</code> feature, multi-turn conversations can quickly consume available context. As a result, the number of back-and-forth turns in a single conversation may be limited due to the growing context size.</p>"},{"location":"server/apps/anythingLLM/#setup","title":"Setup","text":""},{"location":"server/apps/anythingLLM/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Install and set up AnythingLLM from their GitHub or website.</li> </ol>"},{"location":"server/apps/anythingLLM/#configure-anythingllm-to-use-lemonade","title":"Configure AnythingLLM to Use Lemonade","text":"<ol> <li>In the bottom of the left menu, click on the wrench icon to \"Open Settings\".</li> <li>Under the menu \"AI Providers\", click \"LLM\".</li> <li>     Select \"Generic OpenAI\" and enter the following info:      SettingValue Base URL<code>http://localhost:8000/api/v1</code> API Key<code>-</code> Chat Model Name<code>Qwen-1.5-7B-Chat-Hybrid</code> Token context window<code>3000</code> Max Tokens<code>3000</code> </li> <li>In the bottom left, click the back button to exit.</li> <li>In the left menu, click \"New Workspace\" and give it a name.</li> <li>Where you see your new workspace, click the gear icon to open the \"Workspace Settings\"</li> <li>In the top menu of the window that opens, click on \"Agent Configuration\"</li> <li>Under Chat Settings, select Generic OpenAI and click save.</li> <li>Under Workspace Agent LLM Provider, select \"Generic OpenAI\" and click save.</li> </ol>"},{"location":"server/apps/anythingLLM/#usage-with-agent","title":"Usage with @agent","text":""},{"location":"server/apps/anythingLLM/#overview_1","title":"Overview","text":"<p>Agents are capable of scraping websites, listing and summarizing documents, searching the web, creating charts, and even saving files to your desktop or their own memory.</p> <p>To start an agent session, simply go to any workspace and type <code>@agent &lt;your prompt&gt;</code>. To exit the session, just type <code>exit</code>.</p>"},{"location":"server/apps/anythingLLM/#agent-skills","title":"Agent Skills","text":"<p>You may turn on and off specific <code>Agent Skills</code> by going to your <code>Workspace Settings</code> \u2192 <code>Agent Configuration</code> \u2192 <code>Configure Agent Skills</code>.</p> <p>Available agent skills include:</p> <ul> <li>RAG &amp; long-term memory</li> <li>View and summarize documents</li> <li>Scrape Websites</li> <li>Generate &amp; save files to browser</li> <li>Generate Charts</li> <li>Web Search</li> <li>SQL Connector</li> </ul>"},{"location":"server/apps/anythingLLM/#examples","title":"Examples","text":"<p>Here are some examples on how you can interact with Anything LLM agents:</p> <ul> <li>Rag &amp; long-term memory<ul> <li><code>@agent My name is Dr Lemon. Remember this in our next conversation</code></li> <li>Then, on a follow up chat you can ask <code>@agent What is my name according to your memory?</code></li> </ul> </li> <li>Scrape Websites<ul> <li><code>@agent Scrape this website and tell me what are the two ways of installing lemonade https://github.com/lemonade-sdk/lemonade/blob/main/docs/server/README.md</code></li> </ul> </li> <li>Web Search (enable skill before trying)<ul> <li><code>@agent Search the web for the best place to buy shoes</code></li> </ul> </li> </ul> <p>You can find more details about agent usage here.</p>"},{"location":"server/apps/anythingLLM/#additional-resources","title":"Additional Resources","text":"<ul> <li>AnthingLLM Website</li> <li>AnythingLLM GitHub</li> <li>AnythingLLM Documentation</li> </ul>"},{"location":"server/apps/codeGPT/","title":"CodeGPT with VS Code","text":""},{"location":"server/apps/codeGPT/#overview","title":"Overview","text":"<p>CodeGPT Chat is an AI-powered chatbot designed to assist developers with coding tasks directly within their preferred integrated development environments (IDEs), for example, VS Code.</p>"},{"location":"server/apps/codeGPT/#expectations","title":"Expectations","text":"<p>We have found that the <code>Qwen-1.5-7B-Chat-Hybrid</code> model is the best Hybrid model available for coding. It is good at chatting with a few files at a time in your codebase to learn more about them. It can also make simple code editing suggestions pertaining to a few lines of code at a time.</p> <p>However, we do not recommend using this model for analyzing large codebases at once or making large or complex file edits.</p>"},{"location":"server/apps/codeGPT/#setup","title":"Setup","text":""},{"location":"server/apps/codeGPT/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/codeGPT/#install-codegpt-in-vs-code","title":"Install CodeGPT in VS Code","text":"<p>The following instructions are based off CodeGPT provided instructions found here.</p> <ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search \"CodeGPT: Chat &amp; AI Agents\" in the Extensions Marketplace search bar.</li> <li>Select the CodeGPT extension and click install.</li> </ol> <p>This will add a CodeGPT tab to your VS Code Activity Bar.</p>"},{"location":"server/apps/codeGPT/#add-lemonade-server-to-codegpt","title":"Add Lemonade Server to CodeGPT","text":"<p>Note: The following instructions are based on instructions from CodeGPT found here.</p> <ol> <li>Open the CodeGPT tab in your VS Code Activity Bar.</li> <li>Sign Up or Sign into your account.</li> <li>In the model dropdown menu and click \"View More\".</li> <li>Select the tab: \"LLMs Cloud model\"</li> <li>Under \"All Models\", set the following:    FieldValue Select Provider:<code>Custom</code> Select Model: <code>Qwen-1.5-7B-Chat-Hybrid</code> </li> <li>Click \"Change connection settings\" and enter the following information:      FieldValue API Key<code>-</code> Custom Link<code>http://localhost:8000/api/v1/api/v1</code> </li> </ol>"},{"location":"server/apps/codeGPT/#usage","title":"Usage","text":"<p>Note: see the CodeGPT user guide to learn about all of their features.</p> <p>To try out CodeGPT:</p> <ul> <li>Open the CodeGPT tab in your VS Code Activity Bar, and in the chat box, type a question about your code. Use the <code>#</code> symbol to specify a file.</li> <li>Example: \"What's the fastest way to install lemonade in #getting_started.md?\"</li> <li>Use /Fix to find and fix a minor bug.</li> <li>Use /Document to come up with docstrings and comments for a file.</li> <li>Use /UnitTest to make a  test file.</li> </ul>"},{"location":"server/apps/continue/","title":"Continue.dev","text":"<p>Continue.dev is an open-source AI code assistant for VS Code and JetBrains that enables developers to leverage local AI models through Lemonade Server for code generation, editing, and chat capabilities.</p>"},{"location":"server/apps/continue/#prerequisites","title":"Prerequisites","text":"<p>Before using Continue.dev with Lemonade Server, ensure you have:</p> <ul> <li>IDE: Visual Studio Code (1.80+) or JetBrains IDE</li> <li>Lemonade Server: Install and set up following the Getting Started guide</li> <li>Server running: Ensure Lemonade Server is running on <code>http://localhost:8000</code></li> <li>Models installed: At least one model from the supported models list downloaded locally; this should match the one you will pick below from Continue Hub</li> </ul>"},{"location":"server/apps/continue/#installation","title":"Installation","text":""},{"location":"server/apps/continue/#installing-continue-extension","title":"Installing Continue Extension","text":"<ol> <li>Open VS Code</li> <li>Navigate to the Extensions marketplace</li> <li>Search for \"Continue\" </li> <li>Install the Continue extension</li> </ol>"},{"location":"server/apps/continue/#configuring-with-continue-hub","title":"Configuring with Continue Hub","text":"<p>Continue Hub provides pre-configured model setups that work immediately with Lemonade Server. Note: The models shown on Continue Hub must be installed on your local machine to function properly.</p> <ol> <li> <p>Install Models Locally: First, ensure you have the desired models installed using the lemonade-server CLI:    <pre><code>lemonade-server pull &lt;model-name&gt;\n</code></pre>    For example: <code>lemonade-server pull Qwen2.5-7B-Instruct-Hybrid</code></p> </li> <li> <p>Access Lemonade Models: Visit hub.continue.dev</p> </li> <li>Select Configuration: Browse available Lemonade models and configurations - all models listed are from the supported models list</li> </ol> <p></p> <ol> <li>Add to Continue: Click the plus icon <code>(+)</code> on your chosen configuration to add to your assistant - see the Continue.dev assistants guide for detailed setup instructions</li> <li>Automatic Setup: The configuration is automatically added to your Continue extension upon signing in via the extension in your chosen IDE.</li> </ol> <p></p>"},{"location":"server/apps/continue/#working-with-continuedev","title":"Working with Continue.dev","text":"<p>Continue.dev provides three interaction modes for different development tasks. See the Continue documentation for detailed mode descriptions.</p>"},{"location":"server/apps/continue/#mode-selection-guide","title":"Mode Selection Guide","text":"<ul> <li>Chat: Code explanations, debugging discussions, architecture planning</li> <li>Agent: Multi-file refactoring, large-scale changes across projects</li> </ul>"},{"location":"server/apps/continue/#examples","title":"Examples","text":""},{"location":"server/apps/continue/#example-1-chat-mode-building-an-asteroids-game","title":"Example 1: Chat Mode - Building an Asteroids Game","text":"<p>Scenario: You want to quickly prototype a game concept through conversation with the AI.</p> <p>Chat Conversation:</p> <p>User: I want to create an asteroids game using PyGame. What guidelines should I follow in the code to do so?</p> <pre><code>Model: Creating an Asteroids game using PyGame is a fun and educational project! Below are some guidelines you should follow to structure your code effectively:\n\n### 1. Initialize PyGame\n- Always start by initializing PyGame and setting up the display.\n\n### 2. Define Game Constants\n- Define constants for screen dimensions, colors, speeds, and other game settings.\n\n### 3. Create Classes for Game Objects\n- Define classes for different game objects like the Player, Asteroids, Bullets, etc.\n- Each class should handle its own rendering and movement.\n\n### 4. Implement Game Loop\n- The main game loop should handle updating the game state and rendering the screen.\n\n### 5. Handle User Input\n- Implement a way to handle keyboard and mouse inputs to control the player's ship.\n\n### 6. Collision Detection\n- Implement collision detection between bullets and asteroids, and between the player ship and asteroids.\n\n### 7. Game Logic\n- Implement game logic for spawning asteroids, shooting bullets, and updating the game state.\n\n### 8. Score and Game Over Conditions\n- Keep track of the player's score and implement conditions for game over (e.g., when the player's ship collides with an asteroid).\n\nHere's a simple structure to get you started:\n\n# &lt;model generated code here&gt;...\n\n### Explanation of the Code:\n1. **Initialize PyGame**: Set up the display and define constants.\n2. **Player Class**: Handles the player's movement and drawing.\n3. **Asteroid Class**: Handles the asteroid's movement and drawing.\n4. **Bullet Class**: Handles the bullet's movement and drawing.\n5. **Game Loop**: Processes input, updates game state, and renders the screen.\n\nThis code provides a basic framework for an Asteroids game. You can expand on it by adding more features like collision detection, scoring, and different levels of difficulty.\n</code></pre> <p>User: what change could we make to ensure that the ship is unchanged, but has a yellow trail effect when it moves?</p> <pre><code>Model: To create a yellow trail effect for the player's ship, you can maintain a list of the ship's previous positions and draw small circles or short lines at these positions. This will give the appearance of a trailing effect as the ship moves.\n\nHere's how you can modify the `Player` class and the main game loop to include this feature:\n\n# &lt;model generated&gt;...\n\n### Explanation of Changes:\n1. **Player Class**:\n   - Added a `trail` list to store the previous positions of the ship.\n   - Updated the `move` method to append the current position to the `trail` list and remove the oldest position if the trail exceeds the specified length (`TRAIL_LENGTH`).\n   - Updated the `draw` method to draw small yellow circles at each position in the `trail` list.\n\n2. **Game Loop**:\n   - No changes were needed in the main game loop as the trail effect is handled within the `Player` class.\n\nThis code will create a yellow trail effect behind the player's ship as it moves across the screen. You can adjust the `TRAIL_LENGTH` constant to make the trail longer or shorter.\n</code></pre> <p>Through this conversational approach, the model helps build and refine the game iteratively, explaining decisions and accommodating new requirements as they arise.</p> <p></p> <p>After running the <code>pip install pygame</code> command to setup dependencies, and applying the changes to <code>main.py</code>, the game can be run using <code>python .\\main.py</code>. This results should resemble the following:</p> <p></p>"},{"location":"server/apps/continue/#example-2-agent-mode-converting-callbacks-to-asyncawait","title":"Example 2: Agent Mode - Converting Callbacks to Async/Await","text":"<p>Scenario: You have older code using callback patterns that needs to be modernized to async/await for better readability and error handling.</p> <p>Agent Task:</p> <p>Convert all callback-based functions in <code>@run.js</code> use async/await: - Convert callback patterns to async/await - Add proper try/catch error handling - Update all calling code to use await - Maintain the same functionality</p>"},{"location":"server/apps/continue/#example-conversion","title":"Example Conversion","text":"<p>Before (callback pattern): <pre><code>const db = require('./db');\n\nfunction processUser(user, callback) {\n  setTimeout(() =&gt; {\n    if (!user || !user.id) {\n      callback(new Error('Invalid user'), null);\n    } else {\n      callback(null, { ...user, processed: true });\n    }\n  }, 100);\n}\n\nfunction fetchUserData(userId, callback) {\n  db.query('SELECT * FROM users WHERE id = ?', [userId], (err, result) =&gt; {\n    if (err) {\n      callback(err, null);\n    } else {\n      processUser(result, (processErr, processed) =&gt; {\n        if (processErr) {\n          callback(processErr, null);\n        } else {\n          callback(null, processed);\n        }\n      });\n    }\n  });\n}\n</code></pre></p> <p>After (async/await): <pre><code>const db = require('./db');\n\nasync function processUser(user) {\n  return new Promise((resolve, reject) =&gt; {\n  setTimeout(() =&gt; {\n    if (!user || !user.id) {\n        reject(new Error('Invalid user'));\n    } else {\n        resolve({ ...user, processed: true });\n    }\n  }, 100);\n  });\n}\n\nasync function fetchUserData(userId) {\n  try {\n    const result = await new Promise((resolve, reject) =&gt; {\n  db.query('SELECT * FROM users WHERE id = ?', [userId], (err, result) =&gt; {\n    if (err) {\n          reject(err);\n    } else {\n          resolve(result);\n        }\n      });\n    });\n\n    const processedUser = await processUser(result);\n    return processedUser;\n  } catch (err) {\n    throw err;\n  }\n}\n</code></pre></p> <p>Agent Actions: 1. Identifies all callback-based functions in service files 2. Converts each to async/await syntax 3. Updates error handling to use try/catch blocks 4. Updates all calling code to use await 5. Ensures promise chains are properly handled</p> <p>The Agent intelligently handles nested callbacks, error propagation, and ensures all calling code is updated consistently.</p> <p></p>"},{"location":"server/apps/continue/#best-practices","title":"Best Practices","text":""},{"location":"server/apps/continue/#setup-configuration","title":"Setup &amp; Configuration","text":"<ul> <li>Install Lemonade Server following the setup guide before starting development</li> <li>Install models locally using <code>lemonade-server pull &lt;model-name&gt;</code> - see the supported models list for available options</li> <li>Pre-load models in Lemonade Server before coding sessions and keep the server running throughout development</li> <li>For agent mode with GGUF models, use increased context size: <code>lemonade-server serve --ctx-size 8192</code></li> <li>Build custom assistants tailored to specific languages/domains (e.g., JavaScript/TypeScript for Node.js projects)  </li> <li>Avoid adding too many models with the same mode to prevent suboptimal model switching</li> <li>See continue.dev customization for effective scoping strategies</li> </ul>"},{"location":"server/apps/continue/#development-workflow","title":"Development Workflow","text":"<ul> <li>Start fresh conversations for new features; clear chat history when switching tasks</li> <li>Include only relevant code in prompts to maintain focus</li> <li>Structure prompts with clear task descriptions, specific requirements, and technical constraints</li> <li>Use <code>@</code> symbol to invoke agent mode for multi-file operations</li> <li>Evolve prompts from vague (\"Create a game\") to specific (\"Create an Asteroids game in Python using Pygame, under 300 lines, with ship controls and asteroid splitting\")</li> <li>Generate initial implementation, test immediately, then refine with targeted prompts</li> <li>Take advantage of unlimited iterations with local models for continuous improvement</li> </ul>"},{"location":"server/apps/continue/#common-issues","title":"Common Issues","text":"<p>Issue: Model not appearing in Continue Solution: Verify Lemonade Server is running and model is loaded locally. Check the supported models list and install with <code>lemonade-server pull &lt;model-name&gt;</code></p> <p>Issue: Slow response times Solution: Ensure model is pre-loaded, check available RAM</p> <p>Issue: Missing error handling in generated code Solution: Explicitly request \"with comprehensive error handling\"</p> <p>Issue: Inconsistent code style Solution: Provide example of desired style in prompt</p>"},{"location":"server/apps/continue/#resources","title":"Resources","text":"<ul> <li>Lemonade Server Setup Guide</li> <li>Lemonade Server Supported Models</li> <li>Continue.dev Documentation</li> <li>Continue Hub</li> <li>Lemonade Discord</li> <li>Example Projects</li> </ul>"},{"location":"server/apps/lm-eval/","title":"Using LM-Evaluation-Harness with Lemonade","text":"<p>This guide demonstrates how to use Lemonade with LM-Evaluation-Harness (lm-eval) to evaluate language model performance across a variety of standardized benchmarks. Whether you're comparing different model implementations or validating model capabilities, lm-eval provides a comprehensive framework for model assessment. Refer to Lemonade Server to learn more about the server interface used by lm-eval for evaluations.</p>"},{"location":"server/apps/lm-eval/#what-is-lm-evaluation-harness","title":"What is LM-Evaluation-Harness?","text":"<p>LM-Evaluation-Harness (often called <code>lm-eval</code>) is an open-source framework for evaluating language models across a wide variety of tasks and benchmarks. Developed by EleutherAI, it has become a standard tool in the AI research community for consistent evaluation of language model capabilities.</p> <p>The framework supports evaluating models on more than 200 tasks and benchmarks, including popular ones such as:</p> <ul> <li>MMLU (Massive Multitask Language Understanding)</li> <li>GSM8K (Grade School Math)</li> <li>HumanEval (Code generation)</li> <li>TruthfulQA</li> <li>WinoGrande</li> <li>HellaSwag</li> <li>And many others...</li> </ul>"},{"location":"server/apps/lm-eval/#advantages-of-using-lm-eval-for-accuracy-measurement","title":"Advantages of Using lm-eval for Accuracy Measurement","text":"<ul> <li>Standardization: Provides a consistent methodology for comparing different models, ensuring fair comparisons across the industry.</li> <li>Community adoption: Used by major research labs, companies, and the open-source community including Hugging Face, Anthropic, and others.</li> <li>Comprehensive evaluation: Covers a wide range of capabilities from factual knowledge to reasoning.</li> <li>Open-source: Transparent methodology that's peer-reviewed by the AI research community.</li> <li>Regular updates: Continuously updated with new benchmarks and evaluation methods.</li> <li>Reproducibility: Enables reproducible research results across different models and implementations.</li> <li>Cross-implementation compatibility: Works with multiple model implementations (llama.cpp, OpenAI API, Hugging Face, etc.) enabling direct comparison of different implementations of the same model.</li> </ul>"},{"location":"server/apps/lm-eval/#running-lm-eval-with-lemonade","title":"Running lm-eval with Lemonade","text":"<p>Lemonade supports integration with lm-eval through its local LLM server. The basic workflow involves:</p> <ol> <li>Setting up the environment.</li> <li>Starting the Lemonade server.</li> <li>Loading a model via the API.</li> <li>Running lm-eval tests against the model through the lemonade server.</li> </ol>"},{"location":"server/apps/lm-eval/#step-1-environment-setup-and-installation","title":"Step 1: Environment Setup and Installation","text":"<p>Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p>"},{"location":"server/apps/lm-eval/#step-2-start-the-lemonade-server","title":"Step 2: Start the Lemonade Server","text":"<p>In a terminal with your environment activated, run the following command:</p> <pre><code>lemonade-server-dev serve\n</code></pre> <p>This starts a local LLM server on port 8000 by default.</p>"},{"location":"server/apps/lm-eval/#step-3-load-a-model","title":"Step 3: Load a Model","text":"<p>Use the following PowerShell command to load a model into the server:</p> <pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> <p>Where:</p> <ul> <li><code>checkpoint</code> can be changed to use other from Hugging Face (e.g., \"meta-llama/Llama-3.2-3B-Instruct\")</li> <li><code>recipe</code> can be changed to use different backends (e.g., \"oga-cpu\" for CPU inference on OnnxRuntime GenAI, \"oga-hybrid\" for AMD Ryzen\u2122 AI acceleration). For more information on Lemonade recipes, see the Lemonade API ReadMe.</li> </ul>"},{"location":"server/apps/lm-eval/#step-4-run-lm-eval-tests","title":"Step 4: Run lm-eval Tests","text":"<p>Now that the model is loaded, open a new PowerShell terminal, activate your environment, and run lm-eval tests using the following command:</p> <pre><code>lm_eval --model local-completions --tasks mmlu_abstract_algebra --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre> <p>Where: - Change <code>--tasks</code> as needed to run other tests (e.g., <code>--tasks gsm8k</code>, <code>--tasks wikitext</code>, etc.) For detailed tasks visit lm-eval - <code>checkpoint name</code> should match the model name loaded in step 2</p>"},{"location":"server/apps/lm-eval/#types-of-tests-in-lm-eval","title":"Types of Tests in lm-eval","text":"<p>The framework implements three primary evaluation methodologies that use different capabilities of language models:</p>"},{"location":"server/apps/lm-eval/#1-log-probability-based-tests-eg-mmlu","title":"1. Log Probability-Based Tests (e.g., MMLU)","text":"<p>These tests evaluate a model's ability to assign probabilities to different possible answers. The model predicts which answer is most likely based on conditional probabilities.</p> <p>Example: In MMLU (Massive Multitask Language Understanding), the model is given a multiple-choice question and must assign probabilities to each answer choice. The model's performance is measured by how often it assigns the highest probability to the correct answer.</p>"},{"location":"server/apps/lm-eval/#commands-to-log-probability-based-tests","title":"Commands to Log Probability-Based Tests","text":"<p>Step 1: Environment setup and installation - Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p> <p>Step 2: Start the Lemonade Server.</p> <p>In a terminal with your environment activated, run the following command:</p> <p><pre><code>lemonade-server-dev serve\n</code></pre> Step 3: Load a Model</p> <p><pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> Step 4: Run MMLU Tests</p> <pre><code>lm_eval --model local-completions --tasks mmlu_abstract_algebra --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre>"},{"location":"server/apps/lm-eval/#2-rolling-log-probability-tests-eg-wikitext","title":"2. Rolling Log Probability Tests (e.g., WikiText)","text":"<p>These tests evaluate a model's ability to predict text by measuring the perplexity on held-out data. The model assigns probabilities to each token in a sequence, and performance is measured by how well it predicts the actual next tokens.</p> <p>Example: In perplexity benchmarks like WikiText, the model is evaluated on how well it can predict each token in a document, using a rolling window approach for longer contexts.</p>"},{"location":"server/apps/lm-eval/#commands-to-log-probability-based-tests_1","title":"Commands to Log Probability-Based Tests","text":"<p>Step 1: Environment setup and installation - Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p> <p>Step 2: Start the Lemonade Server.</p> <p>In a terminal with your environment activated, run the following command:</p> <p><pre><code>lemonade-server-dev serve\n</code></pre> Step 3: Load a Model</p> <p><pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> Step 4: Run Wikitext Tests</p> <pre><code>lm_eval --model local-completions --tasks wikitext --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre>"},{"location":"server/apps/lm-eval/#3-generation-based-tests-eg-gsm8k","title":"3. Generation-Based Tests (e.g., GSM8K)","text":"<p>These tests evaluate a model's ability to generate full responses to prompts. The model generates text that is then evaluated against reference answers or using specific metrics.</p> <p>Example: In GSM8K (Grade School Math), the model is given a math problem and must generate a step-by-step solution. Performance is measured by whether the final answer is correct.</p>"},{"location":"server/apps/lm-eval/#commands-to-log-probability-based-tests_2","title":"Commands to Log Probability-Based Tests","text":"<p>Step 1: Environment setup and installation - Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p> <p>Step 2: Start the Lemonade Server.</p> <p>In a terminal with your environment activated, run the following command:</p> <p><pre><code>lemonade-server-dev serve\n</code></pre> Step 3: Load a Model</p> <p><pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> Step 4: Run GSM8k Tests</p> <pre><code>lm_eval --model local-completions --tasks gsm8k --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre>"},{"location":"server/apps/lm-eval/#interpreting-results","title":"Interpreting Results","text":"<p>lm-eval provides detailed results for each benchmark, typically including:</p> <ul> <li>Accuracy: Percentage of correct answers.</li> <li>Exact Match: For generation tasks, whether the generated answer exactly matches the reference.</li> <li>F1 Score: For certain tasks, measuring the overlap between generated and reference answers.</li> <li>Perplexity: For language modeling tasks, measuring how well the model predicts text.</li> <li>Group breakdowns: For some benchmarks, performance across different categories or question types.</li> </ul> <p>Results are provided in a structured format at the end of evaluation, with both detailed and summary statistics.</p>"},{"location":"server/apps/lm-eval/#future-work","title":"Future Work","text":"<ul> <li>Integrate lm-eval as a Lemonade tool: Direct integration into the Lemonade CLI ecosystem.</li> </ul>"},{"location":"server/apps/lm-eval/#references","title":"References","text":"<ul> <li>LM-Evaluation-Harness GitHub Repository</li> <li>EleutherAI Documentation </li> </ul>"},{"location":"server/apps/mindcraft/","title":"Mindcraft","text":""},{"location":"server/apps/mindcraft/#overview","title":"Overview","text":"<p>Mindcraft is an open-source project that creates Minecraft bots powered by large language models (LLMs) to engage with the game and its players. This readme will demonstrate how to integrate Lemonade to use local LLMs with Mindcraft.</p>"},{"location":"server/apps/mindcraft/#expectations","title":"Expectations","text":"<p>We found the <code>Qwen-1.5-7B-Chat-Hybrid</code> model to be the most effective for this task, delivering fast responses with higher accuracy. However, as a smaller model running locally, with a limited context length, it may occasionally struggle with certain requests\u2014for instance, it might attempt to build a structure, but the result may not be correct. For more detailed information, please refer to the Data Insights section.</p>"},{"location":"server/apps/mindcraft/#setup","title":"Setup","text":""},{"location":"server/apps/mindcraft/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Obtain a copy of Minecraft from Microsoft. 1.20.4 of the JAVA Edition is required for this. You can obtain that version by following these instructions.</li> <li>Install Node.js (at least v14).</li> </ol>"},{"location":"server/apps/mindcraft/#environment-setup","title":"Environment Setup","text":"<p>Clone Mindcraft from GitHub:</p> <ul> <li><code>git checkout 07ea071ac3b0d4954d62b09d881c38a06bc2a589</code>: this will ensure the code base is equal to where this test was performed.</li> </ul> <p>In the clone of the <code>mindcraft</code> repository:</p> <ul> <li>Rename the file <code>keys.example.json</code> to <code>keys.json</code>. You do not need to edit the contents of this JSON file.</li> <li>Update <code>keys.json</code> by adding any text to the value of <code>OPENAI_API_KEY</code>.</li> </ul> <pre><code>{\n    \"OPENAI_API_KEY\": \"&lt;put any text here&gt;\",\n    \"OPENAI_ORG_ID\": \"\",\n    \"GEMINI_API_KEY\": \"\",\n    \"ANTHROPIC_API_KEY\": \"\",\n    \"REPLICATE_API_KEY\": \"\",\n    \"GROQCLOUD_API_KEY\": \"\",\n    \"HUGGINGFACE_API_KEY\": \"\",\n    \"QWEN_API_KEY\": \"\",\n    \"XAI_API_KEY\": \"\",\n    \"MISTRAL_API_KEY\": \"\",\n    \"DEEPSEEK_API_KEY\": \"\",\n    \"NOVITA_API_KEY\": \"\",\n    \"OPENROUTER_API_KEY\": \"\"\n}\n</code></pre> <ul> <li>In a terminal/command prompt, run <code>npm install</code> from the cloned Mindcraft directory.</li> <li>Replace the contents of the file <code>andy.json</code> with the following:</li> </ul> <pre><code>{\n    \"name\": \"Andy\",\n    \"model\": {\n        \"model\": \"Qwen-1.5-7B-Chat-Hybrid\",\n        \"url\": \"http://localhost:8000/api/v1\",\n        \"params\": {\n            \"temperature\": 0.5\n        }\n    },\n    \"modes\": {\n        \"self_preservation\": true,\n        \"unstuck\": true,\n        \"cowardice\": false,\n        \"self_defense\": true,\n        \"hunting\": true,\n        \"item_collecting\": true,\n        \"torch_placing\": true,\n        \"elbow_room\": true,\n        \"idle_staring\": true,\n        \"cheat\": true\n    }\n}\n</code></pre> <ul> <li>Find the line in <code>src/models/prompter.js</code> that says:</li> </ul> <pre><code>else if (profile.model.includes('gpt') || profile.model.includes('o1')|| profile.model.includes('o3'))\n</code></pre> <ul> <li>... and replace it with the following:</li> </ul> <pre><code>else if ([\n    'Qwen-1.5-7B-Chat-Hybrid',\n    'Llama-3.2-1B-Instruct-Hybrid'\n].includes(profile.model) || profile.model.includes('gpt') || profile.model.includes('o1') || profile.model.includes('o3'))\n</code></pre>"},{"location":"server/apps/mindcraft/#launching-everything","title":"Launching Everything","text":"<ul> <li>Start Lemonade Server by double-clicking the desktop icon \ud83c\udf4b.</li> <li>Start a Minecraft world and open it to LAN on localhost port <code>55916</code>.<ul> <li>This is done by pressing the ESC button to open the menu, then click \"Open to LAN\" and enter the Port Number: <code>55916</code>.</li> <li>Click \"Start LAN World\". For instructions on how Open to LAN, see the section \"Hosting a LAN Server\" for the JAVA Edition in this wiki article.</li> </ul> </li> <li>Run <code>node main.js</code> from the installed directory.</li> <li>In Minecraft, to give the agent commands, press <code>t</code> and enter the command. For example:<ul> <li>\"come here\"</li> <li>\"hunt pigs\" - you and the agent must be close to some pigs to do this.</li> </ul> </li> </ul>"},{"location":"server/apps/mindcraft/#model-configurations","title":"Model Configurations","text":"<p>Lemonade models tested:</p> <ul> <li>Llama-3.2-1B-Instruct-Hybrid</li> <li>Qwen-1.5-7B-Chat-Hybrid \ud83d\udc4d</li> </ul>"},{"location":"server/apps/mindcraft/#challenges-and-observations","title":"Challenges and Observations","text":"<ol> <li>The current MindCraft configuration has a tendency to send very large context that is resent. The context will include examples of behaviors that may not be necessary, generating over 2100 input tokens for simple commands. Further testing would be required to understand model behavior based on reduced context size.</li> <li>Frequent token limit breaches resulting in timeouts or incomplete responses due to the aforementioned context size problem. Once the token maximum content ceiling is raised, further testing would be prudent using the same <code>Qwen-1.5-7B-Chat-Hybrid</code> model as a baseline and using it to test other models such as DeepSeek and Llama variants. content</li> <li>High GPU resource consumption during model inference, impacting system performance.</li> </ol>"},{"location":"server/apps/mindcraft/#results","title":"Results","text":"<p>The <code>Qwen-1.5-7B-Chat-Hybrid</code> model showed the most potential, responding to commands and attempting construction tasks, though with limited accuracy.</p>"},{"location":"server/apps/mindcraft/#recommendations","title":"Recommendations","text":"<ol> <li>Optimize examples sent to Lemonade for conciseness to reduce token usage.</li> <li>Reduce input context to prevent model overload.</li> </ol>"},{"location":"server/apps/mindcraft/#data-insights","title":"Data Insights","text":"<p>The following are examples of requests made by the Mindcraft software to the Lemonade Server and how the tokens were interpreted. These examples are taken from an initial game stage, including the first request sent to the Lemonade Server and a subsequent user chat that says, \"come here.\" The purpose is to show how large the context is that is being sent. This could be optimized for more efficient performance and results.</p> <ul> <li>Initial Payload</li> </ul> <pre><code>{\"model\":\"Qwen-1.5-7B-Chat-Hybrid\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\\n\\n\\nSTATS\\n- Position: x: 6.50, y: -60.00, z: 28.50\\n- Gamemode: creative\\n- Health: 20 / 20\\n- Hunger: 20 / 20\\n- Biome: plains\\n- Weather: Clear\\n- Block Below: grass_block\\n- Block at Legs: air\\n- Block at Head: air\\n- First Solid Block Above Head: none\\n- Time: Afternoon- Current Action: Idle\\n- Nearby Human Players: Transpier\\n- Nearby Bot Players: None.\\nAgent Modes:\\n- self_preservation(ON)\\n- unstuck(ON)\\n- cowardice(ON)\\n- self_defense(ON)\\n- hunting(ON)\\n- item_collecting(ON)\\n- torch_placing(ON)\\n- elbow_room(ON)\\n- idle_staring(ON)\\n- cheat(ON)\\n\\n\\n\\nINVENTORY: Nothing\\nWEARING: Nothing\\n\\n\\n*COMMAND DOCS\\n You can use the following commands to perform actions and get information about the world. \\n    Use the commands with the syntax: !commandName or !commandName(\\\"arg1\\\", 1.2, ...) if the command takes arguments.\\n\\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\\n!stats: Get your bot's location, health, hunger, and time of day.\\n!inventory: Get your bot's inventory.\\n!nearbyBlocks: Get the blocks near the bot.\\n!craftable: Get the craftable items with the bot's inventory.\\n!entities: Get the nearby players and entities.\\n!modes: Get all available modes and their docs and see which are on/off.\\n!savedPlaces: List all saved locations.\\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\\nParams:\\ntargetItem: (string) The item that we are trying to craft\\nquantity: (number) The quantity of the item that we are trying to craft\\n!help: Lists all available commands and their descriptions.\\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\\nParams:\\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\\n!stop: Force stop all actions and commands that are currently executing.\\n!stfu: Stop all chatting and self prompting, but continue current action.\\n!restart: Restart the agent process.\\n!clearChat: Clear the chat history.\\n!goToPlayer: Go to the given player.\\nParams:\\nplayer_name: (string) The name of the player to go to.\\ncloseness: (number) How close to get to the player.\\n!followPlayer: Endlessly follow the given player.\\nParams:\\nplayer_name: (string) name of the player to follow.\\nfollow_dist: (number) The distance to follow from.\\n!goToCoordinates: Go to the given x, y, z location.\\nParams:\\nx: (number) The x coordinate.\\ny: (number) The y coordinate.\\nz: (number) The z coordinate.\\ncloseness: (number) How close to get to the location.\\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\\nParams:\\ntype: (string) The block type to go to.\\nsearch_range: (number) The range to search for the block.\\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\\nParams:\\ntype: (string) The type of entity to go to.\\nsearch_range: (number) The range to search for the entity.\\n!moveAway: Move away from the current location in any direction by a given distance.\\nParams:\\ndistance: (number) The distance to move away.\\n!rememberHere: Save the current location with a given name.\\nParams:\\nname: (string) The name to remember the location as.\\n!goToRememberedPlace: Go to a saved location.\\nParams:\\nname: (string) The name of the location to go to.\\n!givePlayer: Give the specified item to the given player.\\nParams:\\nplayer_name: (string) The name of the player to give the item to.\\nitem_name: (string) The name of the item to give.\\nnum: (number) The number of items to give.\\n!consume: Eat/drink the given item.\\nParams:\\nitem_name: (string) The name of the item to consume.\\n!equip: Equip the given item.\\nParams:\\nitem_name: (string) The name of the item to equip.\\n!putInChest: Put the given item in the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to put in the chest.\\nnum: (number) The number of items to put in the chest.\\n!takeFromChest: Take the given items from the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to take.\\nnum: (number) The number of items to take.\\n!viewChest: View the items/counts of the nearest chest.\\nParams:\\n!discard: Discard the given item from the inventory.\\nParams:\\nitem_name: (string) The name of the item to discard.\\nnum: (number) The number of items to discard.\\n!collectBlocks: Collect the nearest blocks of a given type.\\nParams:\\ntype: (string) The block type to collect.\\nnum: (number) The number of blocks to collect.\\n!craftRecipe: Craft the given recipe a given number of times.\\nParams:\\nrecipe_name: (string) The name of the output item to craft.\\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\\n!smeltItem: Smelt the given item the given number of times.\\nParams:\\nitem_name: (string) The name of the input item to smelt.\\nnum: (number) The number of times to smelt the item.\\n!clearFurnace: Take all items out of the nearest furnace.\\nParams:\\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\\nParams:\\ntype: (string) The block type to place.\\n!attack: Attack and kill the nearest entity of a given type.\\nParams:\\ntype: (string) The type of entity to attack.\\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\\nParams:\\nplayer_name: (string) The name of the player to attack.\\n!goToBed: Go to the nearest bed and sleep.\\n!activate: Activate the nearest object of a given type.\\nParams:\\ntype: (string) The type of object to activate.\\n!stay: Stay in the current location no matter what. Pauses all modes.\\nParams:\\ntype: (number) The number of seconds to stay. -1 for forever.\\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\\nParams:\\nmode_name: (string) The name of the mode to enable.\\non: (bool) Whether to enable or disable the mode.\\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\\nParams:\\nselfPrompt: (string) The goal prompt.\\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action. \\n!startConversation: Start a conversation with a player. Use for bots only.\\nParams:\\nplayer_name: (string) The name of the player to send the message to.\\nmessage: (string) The message to send.\\n!endConversation: End the conversation with the given player.\\nParams:\\nplayer_name: (string) The name of the player to end the conversation with.\\n!digDown: Digs down a specified distance.\\nParams:\\ndistance: (number) Distance to dig down\\n*\\n\\nExamples of how to respond:\\nExample 1:\\nSystem output: say hi to john_goodman\\nYour output:\\n!startConversation(\\\"john_goodman\\\", \\\"Hey John\\\"))\\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\\nYour output:\\nHey John, not much. Just saying hi.\\nUser input: john_goodman: (FROM OTHER BOT)Bye!\\nYour output:\\nBye! !endConversation('john_goodman')\\n\\nExample 2:\\nUser input: miner_32: Hey! What are you up to?\\nYour output:\\nNothing much miner_32, what do you need?\\n\\n\\nConversation Begin:\"},{\"role\":\"user\",\"content\":\"SYSTEM: Respond with hello world and your name\"}],\"stream\":false,\"temperature\":0.7,\"max_tokens\":1900,\"top_p\":0.3}\n</code></pre> <ul> <li>Initial Response</li> </ul> <pre><code>TRACE:    ::1:56880 - HTTP connection made\nTRACE:    ::1:56880 - ASGI [4] Started scope={'type': 'http', 'asgi': {'version': '3.0', 'spec_version': '2.3'}, 'http_version': '1.1', 'server': ('::1', 8000), 'client': ('::1', 56880), 'scheme': 'http', 'root_path': '', 'headers': '&lt;...&gt;', 'state': {}, 'method': 'POST', 'path': '/api/v1/chat/completions', 'raw_path': b'/api/v1/chat/completions', 'query_string': b''}\nTRACE:    ::1:56880 - ASGI [4] Receive {'type': 'http.request', 'body': '&lt;8378 bytes&gt;', 'more_body': False}\nDEBUG:    Input Tokens: 2036\nTRACE:    Input Message: &lt;|im_start|&gt;system\nYou are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\n\n\nSTATS\n- Position: x: 6.50, y: -60.00, z: 28.50\n- Gamemode: creative\n- Health: 20 / 20\n- Hunger: 20 / 20\n- Biome: plains\n- Weather: Clear\n- Block Below: grass_block\n- Block at Legs: air\n- Block at Head: air\n- First Solid Block Above Head: none\n- Time: Afternoon- Current Action: Idle\n- Nearby Human Players: Transpier\n- Nearby Bot Players: None.\nAgent Modes:\n- self_preservation(ON)\n- unstuck(ON)\n- cowardice(ON)\n- self_defense(ON)\n- hunting(ON)\n- item_collecting(ON)\n- torch_placing(ON)\n- elbow_room(ON)\n- idle_staring(ON)\n- cheat(ON)\n\n\n\nINVENTORY: Nothing\nWEARING: Nothing\n\n\n*COMMAND DOCS\nYou can use the following commands to perform actions and get information about the world. \n    Use the commands with the syntax: !commandName or !commandName(\"arg1\", 1.2, ...) if the command takes arguments.\n\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\n!stats: Get your bot's location, health, hunger, and time of day.\n!inventory: Get your bot's inventory.\n!nearbyBlocks: Get the blocks near the bot.\n!craftable: Get the craftable items with the bot's inventory.\n!entities: Get the nearby players and entities.\n!modes: Get all available modes and their docs and see which are on/off.\n!savedPlaces: List all saved locations.\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\nParams:\ntargetItem: (string) The item that we are trying to craft\nquantity: (number) The quantity of the item that we are trying to craft\n!help: Lists all available commands and their descriptions.\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\nParams:\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.     \n!stop: Force stop all actions and commands that are currently executing.\n!stfu: Stop all chatting and self prompting, but continue current action.\n!restart: Restart the agent process.\n!clearChat: Clear the chat history.\n!goToPlayer: Go to the given player.\nParams:\nplayer_name: (string) The name of the player to go to.\ncloseness: (number) How close to get to the player.\n!followPlayer: Endlessly follow the given player.\nParams:\nplayer_name: (string) name of the player to follow.\nfollow_dist: (number) The distance to follow from.\n!goToCoordinates: Go to the given x, y, z location.\nParams:\nx: (number) The x coordinate.\ny: (number) The y coordinate.\nz: (number) The z coordinate.\ncloseness: (number) How close to get to the location.\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\nParams:\ntype: (string) The block type to go to.\nsearch_range: (number) The range to search for the block.\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\nParams:\ntype: (string) The type of entity to go to.\nsearch_range: (number) The range to search for the entity.\n!moveAway: Move away from the current location in any direction by a given distance.\nParams:\ndistance: (number) The distance to move away.\n!rememberHere: Save the current location with a given name.\nParams:\nname: (string) The name to remember the location as.\n!goToRememberedPlace: Go to a saved location.\nParams:\nname: (string) The name of the location to go to.\n!givePlayer: Give the specified item to the given player.\nParams:\nplayer_name: (string) The name of the player to give the item to.\nitem_name: (string) The name of the item to give.\nnum: (number) The number of items to give.\n!consume: Eat/drink the given item.\nParams:\nitem_name: (string) The name of the item to consume.\n!equip: Equip the given item.\nParams:\nitem_name: (string) The name of the item to equip.\n!putInChest: Put the given item in the nearest chest.\nParams:\nitem_name: (string) The name of the item to put in the chest.\nnum: (number) The number of items to put in the chest.\n!takeFromChest: Take the given items from the nearest chest.\nParams:\nitem_name: (string) The name of the item to take.\nnum: (number) The number of items to take.\n!viewChest: View the items/counts of the nearest chest.\nParams:\n!discard: Discard the given item from the inventory.\nParams:\nitem_name: (string) The name of the item to discard.\nnum: (number) The number of items to discard.\n!collectBlocks: Collect the nearest blocks of a given type.\nParams:\ntype: (string) The block type to collect.\nnum: (number) The number of blocks to collect.\n!craftRecipe: Craft the given recipe a given number of times.\nParams:\nrecipe_name: (string) The name of the output item to craft.\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\n!smeltItem: Smelt the given item the given number of times.\nParams:\nitem_name: (string) The name of the input item to smelt.\nnum: (number) The number of times to smelt the item.\n!clearFurnace: Take all items out of the nearest furnace.\nParams:\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\nParams:\ntype: (string) The block type to place.\n!attack: Attack and kill the nearest entity of a given type.\nParams:\ntype: (string) The type of entity to attack.\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\nParams:\nplayer_name: (string) The name of the player to attack.\n!goToBed: Go to the nearest bed and sleep.\n!activate: Activate the nearest object of a given type.\nParams:\ntype: (string) The type of object to activate.\n!stay: Stay in the current location no matter what. Pauses all modes.\nParams:\ntype: (number) The number of seconds to stay. -1 for forever.\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\nParams:\nmode_name: (string) The name of the mode to enable.\non: (bool) Whether to enable or disable the mode.\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\nParams:\nselfPrompt: (string) The goal prompt.\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action.    \n!startConversation: Start a conversation with a player. Use for bots only.\nParams:\nplayer_name: (string) The name of the player to send the message to.\nmessage: (string) The message to send.\n!endConversation: End the conversation with the given player.\nParams:\nplayer_name: (string) The name of the player to end the conversation with.\n!digDown: Digs down a specified distance.\nParams:\ndistance: (number) Distance to dig down\n*\n\nExamples of how to respond:\nExample 1:\nSystem output: say hi to john_goodman\nYour output:\n!startConversation(\"john_goodman\", \"Hey John\"))\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\nYour output:\nHey John, not much. Just saying hi.\nUser input: john_goodman: (FROM OTHER BOT)Bye!\nYour output:\nBye! !endConversation('john_goodman')\n\nExample 2:\nUser input: miner_32: Hey! What are you up to?\nYour output:\nNothing much miner_32, what do you need?\n\n\nConversation Begin:&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nSYSTEM: Respond with hello world and your name&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\nDEBUG:    Active generations: 1\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.start', 'status': 200, 'headers': '&lt;...&gt;'}      \nINFO:     ::1:56880 - \"POST /api/v1/chat/completions HTTP/1.1\" 200 OK\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.body', 'body': '&lt;406 bytes&gt;', 'more_body': True}\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.body', 'body': '&lt;0 bytes&gt;', 'more_body': False} \nTRACE:    ::1:56880 - ASGI [4] Completed\nTRACE:    ::1:56880 - HTTP connection lost\n</code></pre> <ul> <li>Subsequent request:</li> </ul> <pre><code>{\"model\":\"Qwen-1.5-7B-Chat-Hybrid\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\\n\\n\\nSTATS\\n- Position: x: 6.50, y: -60.00, z: 28.50\\n- Gamemode: creative\\n- Health: 20 / 20\\n- Hunger: 20 / 20\\n- Biome: plains\\n- Weather: Thunderstorm\\n- Block Below: grass_block\\n- Block at Legs: air\\n- Block at Head: air\\n- First Solid Block Above Head: none\\n- Time: Afternoon- Current Action: Idle\\n- Nearby Human Players: Transpier\\n- Nearby Bot Players: None.\\nAgent Modes:\\n- self_preservation(ON)\\n- unstuck(ON)\\n- cowardice(ON)\\n- self_defense(ON)\\n- hunting(ON)\\n- item_collecting(ON)\\n- torch_placing(ON)\\n- elbow_room(ON)\\n- idle_staring(ON)\\n- cheat(ON)\\n\\n\\n\\nINVENTORY: Nothing\\nWEARING: Nothing\\n\\n\\n*COMMAND DOCS\\n You can use the following commands to perform actions and get information about the world. \\n    Use the commands with the syntax: !commandName or !commandName(\\\"arg1\\\", 1.2, ...) if the command takes arguments.\\n\\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\\n!stats: Get your bot's location, health, hunger, and time of day.\\n!inventory: Get your bot's inventory.\\n!nearbyBlocks: Get the blocks near the bot.\\n!craftable: Get the craftable items with the bot's inventory.\\n!entities: Get the nearby players and entities.\\n!modes: Get all available modes and their docs and see which are on/off.\\n!savedPlaces: List all saved locations.\\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\\nParams:\\ntargetItem: (string) The item that we are trying to craft\\nquantity: (number) The quantity of the item that we are trying to craft\\n!help: Lists all available commands and their descriptions.\\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\\nParams:\\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\\n!stop: Force stop all actions and commands that are currently executing.\\n!stfu: Stop all chatting and self prompting, but continue current action.\\n!restart: Restart the agent process.\\n!clearChat: Clear the chat history.\\n!goToPlayer: Go to the given player.\\nParams:\\nplayer_name: (string) The name of the player to go to.\\ncloseness: (number) How close to get to the player.\\n!followPlayer: Endlessly follow the given player.\\nParams:\\nplayer_name: (string) name of the player to follow.\\nfollow_dist: (number) The distance to follow from.\\n!goToCoordinates: Go to the given x, y, z location.\\nParams:\\nx: (number) The x coordinate.\\ny: (number) The y coordinate.\\nz: (number) The z coordinate.\\ncloseness: (number) How close to get to the location.\\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\\nParams:\\ntype: (string) The block type to go to.\\nsearch_range: (number) The range to search for the block.\\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\\nParams:\\ntype: (string) The type of entity to go to.\\nsearch_range: (number) The range to search for the entity.\\n!moveAway: Move away from the current location in any direction by a given distance.\\nParams:\\ndistance: (number) The distance to move away.\\n!rememberHere: Save the current location with a given name.\\nParams:\\nname: (string) The name to remember the location as.\\n!goToRememberedPlace: Go to a saved location.\\nParams:\\nname: (string) The name of the location to go to.\\n!givePlayer: Give the specified item to the given player.\\nParams:\\nplayer_name: (string) The name of the player to give the item to.\\nitem_name: (string) The name of the item to give.\\nnum: (number) The number of items to give.\\n!consume: Eat/drink the given item.\\nParams:\\nitem_name: (string) The name of the item to consume.\\n!equip: Equip the given item.\\nParams:\\nitem_name: (string) The name of the item to equip.\\n!putInChest: Put the given item in the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to put in the chest.\\nnum: (number) The number of items to put in the chest.\\n!takeFromChest: Take the given items from the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to take.\\nnum: (number) The number of items to take.\\n!viewChest: View the items/counts of the nearest chest.\\nParams:\\n!discard: Discard the given item from the inventory.\\nParams:\\nitem_name: (string) The name of the item to discard.\\nnum: (number) The number of items to discard.\\n!collectBlocks: Collect the nearest blocks of a given type.\\nParams:\\ntype: (string) The block type to collect.\\nnum: (number) The number of blocks to collect.\\n!craftRecipe: Craft the given recipe a given number of times.\\nParams:\\nrecipe_name: (string) The name of the output item to craft.\\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\\n!smeltItem: Smelt the given item the given number of times.\\nParams:\\nitem_name: (string) The name of the input item to smelt.\\nnum: (number) The number of times to smelt the item.\\n!clearFurnace: Take all items out of the nearest furnace.\\nParams:\\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\\nParams:\\ntype: (string) The block type to place.\\n!attack: Attack and kill the nearest entity of a given type.\\nParams:\\ntype: (string) The type of entity to attack.\\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\\nParams:\\nplayer_name: (string) The name of the player to attack.\\n!goToBed: Go to the nearest bed and sleep.\\n!activate: Activate the nearest object of a given type.\\nParams:\\ntype: (string) The type of object to activate.\\n!stay: Stay in the current location no matter what. Pauses all modes.\\nParams:\\ntype: (number) The number of seconds to stay. -1 for forever.\\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\\nParams:\\nmode_name: (string) The name of the mode to enable.\\non: (bool) Whether to enable or disable the mode.\\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\\nParams:\\nselfPrompt: (string) The goal prompt.\\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action. \\n!startConversation: Start a conversation with a player. Use for bots only.\\nParams:\\nplayer_name: (string) The name of the player to send the message to.\\nmessage: (string) The message to send.\\n!endConversation: End the conversation with the given player.\\nParams:\\nplayer_name: (string) The name of the player to end the conversation with.\\n!digDown: Digs down a specified distance.\\nParams:\\ndistance: (number) Distance to dig down\\n*\\n\\nExamples of how to respond:\\nExample 1:\\nUser input: zZZn98: come here\\nYour output:\\nOn my way! !goToPlayer(\\\"zZZn98\\\", 3)\\nSystem output: Arrived at player.\\nYour output:\\nHere!\\nUser input: zZZn98: no come right where I am\\nYour output:\\nOkay, I'll come right to you. !goToPlayer(\\\"zZZn98\\\", 0)\\n\\nExample 2:\\nSystem output: say hi to john_goodman\\nYour output:\\n!startConversation(\\\"john_goodman\\\", \\\"Hey John\\\"))\\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\\nYour output:\\nHey John, not much. Just saying hi.\\nUser input: john_goodman: (FROM OTHER BOT)Bye!\\nYour output:\\nBye! !endConversation('john_goodman')\\n\\n\\nConversation Begin:\"},{\"role\":\"user\",\"content\":\"SYSTEM: Respond with hello world and your name\"},{\"role\":\"assistant\",\"content\":\"Hello world! My name is LLama.\"},{\"role\":\"user\",\"content\":\"Transpier: come here\"},{\"role\":\"assistant\",\"content\":\"On my way! !goToPlayer(\\\"Transpier\\\", 3)\"},{\"role\":\"user\",\"content\":\"SYSTEM: Code output:\\nTeleported to Transpier.\"}],\"stream\":false,\"temperature\":0.7,\"max_tokens\":1900,\"top_p\":0.3}\n</code></pre> <ul> <li>Subsequent Response:</li> </ul> <pre><code>&lt;|im_start|&gt;user\nTranspier: come here&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nOn my way! !goToPlayer(\"Transpier\", 3)&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nSYSTEM: Code output:\nTeleported to Transpier.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\nDEBUG:    Active generations: 1\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.start', 'status': 200, 'headers': '&lt;...&gt;'}      \nINFO:     ::1:56890 - \"POST /api/v1/chat/completions HTTP/1.1\" 200 OK\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.body', 'body': '&lt;381 bytes&gt;', 'more_body': True}\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.body', 'body': '&lt;0 bytes&gt;', 'more_body': False} \nTRACE:    ::1:56890 - ASGI [6] Completed\nTRACE:    ::1:56890 - HTTP connection lost\n</code></pre>"},{"location":"server/apps/open-webui/","title":"Open WebUI","text":"<p>Open WebUI provides a highly polished chat interface in your browser for LLM interaction.</p> <p>This guide walks through how to connect Lemonade Server to Open WebUI and highlights some great features you can start using right away:</p> <ul> <li>Image Uploads to Vision-Language Models (VLMs): Upload images for analysis and interaction with your LLM-powered VLMs.</li> <li>Built-in Python Code Interpreter: Run and test Python code generated by your LLM directly within the interface.</li> <li>Live Preview for Web Development: Preview HTML, CSS, and JavaScript code generated by your LLM using the built-in preview server.</li> </ul>"},{"location":"server/apps/open-webui/#consider-using-gaia","title":"Consider Using GAIA","text":"<p>The team maintains a fork of Open WebUI called GAIA that provides automatic Lemonade integration and a simple Windows installer. You can try it out at https://github.com/amd/gaia. If you choose GAIA, you can skip the installation and configuration sections below and go straight to Using Open WebUI with Lemonade.</p>"},{"location":"server/apps/open-webui/#installing-open-webui","title":"Installing Open WebUI","text":"<ol> <li> <p>We recommend installing Open WebUI into a dedicated conda environment using the following commands. To download conda, see Miniforge.</p> <pre><code>conda create -n webui python=3.11\nconda activate webui\npip install open-webui\n</code></pre> <p>Note: Open WebUI also provides a variety of other installation options, such as Docker, on their GitHub.</p> </li> <li> <p>Run this command to launch the Open WebUI HTTP server:</p> <pre><code>open-webui serve\n</code></pre> </li> <li> <p>In a browser, navigate to http://localhost:8080/</p> </li> <li> <p>Open WebUI will ask you to create a local administrator account. You can fill any username, password, and email you like. Once you are signed in, you will see the chat interface:</p> <p></p> </li> </ol>"},{"location":"server/apps/open-webui/#configuring-open-webui","title":"Configuring Open WebUI","text":"<ol> <li> <p>Install and run Lemonade Server. Download here.</p> </li> <li> <p>Add Lemonade Server as a \"connection\" in Open WebUI using the following steps:</p> <ol> <li>Click the circular user profile button in the top-right of the UI, then click Settings:      Opening the settings menu. </li> <li>Click \"Connections\", then click the \"+\" button:      Navigating to the connection settings. </li> <li>Fill in the URL field with <code>http://localhost:8000/api/v1</code> (unless you're using a different port), API key (this is unused but required, suggest just putting a <code>-</code>), and then click \"Save\".      Filling in the connection details for Lemonade Server. </li> <li>Click \"Save\" in the settings menu, then exit the settings menu.</li> </ol> </li> <li> <p>Apply the suggested settings. These help Open WebUI to be more responsive with local LLMs.</p> <ol> <li>Click the user profile button again, and choose \"Admin Settings\".</li> <li>Click the \"Settings\" tab at the top, then \"Interface\" (which will be on the top or the left, depending on your window size), then disable the following:<ul> <li>Title Generation</li> <li>Follow Up Generation</li> <li>Tags Generation  Admin Settings </li> </ul> </li> <li>Click the \"Save\" button in the bottom right of the page, then return to http://localhost:8080.</li> </ol> </li> </ol>"},{"location":"server/apps/open-webui/#using-open-webui-with-lemonade","title":"Using Open WebUI with Lemonade","text":"<p>Now that everything is configured, you are ready to interact with an LLM!</p>"},{"location":"server/apps/open-webui/#chat","title":"Chat","text":"<ol> <li> <p>Click the dropdown menu in the top-left of the interface. This will display all of the Lemonade models you have installed. Select one to proceed.      Model Selection </p> </li> <li> <p>Enter a message to the LLM and click send (or hit enter). The LLM will take a few seconds to load into memory and then you will see the response stream in.</p> <p> Sending a message </p> <p> LLM response </p> </li> </ol>"},{"location":"server/apps/open-webui/#vision-language-models","title":"Vision Language Models","text":"<p>Vision Language Models (VLMs) can take images as part of their input.</p> <ol> <li> <p>Install a VLM in Lemonade by opening the Lemonade Model Manager:</p> <ol> <li>Open http://localhost:8000 in your browser.</li> <li>Select the Model Management tab.</li> <li> <p>Scroll down until you see a model with the blue <code>VISION</code> label and click the \"+\" button to install it.</p> <p> Installing a VLM </p> </li> </ol> </li> <li> <p>Return to Open WebUI in your browser and select your VLM in the models dropdown menu.</p> </li> <li> <p>Paste an image into the chat box and type a prompt or question about your image. You can also use the \"+\" button in the chat box to upload images.</p> <p> VLM prompt </p> <p> VLM response </p> </li> </ol>"},{"location":"server/apps/open-webui/#python-coding","title":"Python Coding","text":"<p>Open WebUI allows you to run Python code generated by an LLM directly within the interface.</p> <p>Note: only certain Python modules are enabled in Open WebUI. <code>matplotlib</code> is one of our favorites.</p> <ol> <li> <p>Ask the LLM to write some Python, then click the Run button at the top of the Python code block.</p> <p> Ask the LLM to write Python </p> </li> <li> <p>If all goes well, the result of running the Python code will appear below the code block.</p> <p> Python result </p> <p>Note: LLMs often produce incorrect code, so it might take a few chat iterations to fix any bugs. Copy-pasting the Python error message is usually enough to move things along.</p> </li> </ol>"},{"location":"server/apps/open-webui/#html-rendering","title":"HTML Rendering","text":"<p>Open WebUI has a built-in rendering engine for HTML, CSS, and JavaScript pages.</p> <p>Smaller LLMs can produce simple pages with tasteful styling and basic interactivity, while larger LLMs can accomplish tasks like 3D rendering in 3js.</p> <ol> <li> <p>Ask a small LLM to write a simple HTML+CSS page. The preview may pop up automatically, but if it doesn't you can click the Preview button above the HTML code block:</p> <p> HTML rendering </p> </li> <li> <p>Ask a large LLM to create a 3D shape using 3js.</p> <p> 3D rendering </p> </li> </ol>"},{"location":"server/apps/open-webui/#conclusion","title":"Conclusion","text":"<p>These are just a few of our favorite ways to try out LLMs in Open WebUI. There are a lot more features to explore, such as voice interaction and chatting with documents, so be sure to check out the Open WebUI documentation and YouTube content.</p>"},{"location":"server/apps/wut/","title":"<code>wut</code> Terminal Assistant","text":""},{"location":"server/apps/wut/#overview","title":"Overview","text":"<p>The <code>wut</code> terminal assistant uses LLMs to parse your terminal's scrollback, helping you troubleshoot your last command.</p>"},{"location":"server/apps/wut/#expectations","title":"Expectations","text":"<p>We found that <code>wut</code> works nicely with the <code>Llama-3.2-3B-Instruct-Hybrid</code> model.</p> <p>It is not especially convenient to use <code>wut</code> with Windows until the developers remove the requirement for <code>tmux</code>, however we do provide instructions for getting set up on Windows in this guide.</p> <p><code>wut</code> seems to send the entire terminal scrollback to the LLM, which can produce very long prompts that exceed the LLM's context length. We recommend restricting the terminal scrollback or using a fresh <code>tmux</code> session when trying this out.</p>"},{"location":"server/apps/wut/#setup","title":"Setup","text":""},{"location":"server/apps/wut/#prerequisites","title":"Prerequisites","text":""},{"location":"server/apps/wut/#install-lemonade-server","title":"Install Lemonade Server","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/wut/#installing-windows-subsystem-for-linux-wsl","title":"Installing Windows Subsystem for Linux (WSL)","text":"<p><code>wut</code> currently requires a <code>tmux</code> terminal in order to function. We found the simplest way to achieve this on Windows was through the Windows Subsystem for Linux (WSL).</p> <ol> <li>Install Windows Subsystem for Linux.</li> <li>Open the <code>WSL Settings</code> app, navigate to <code>Networking</code>, and make sure the <code>Networking mode</code> is <code>Mirrored</code>. This is required for WSL terminals to be able to see the Lemonade server running in Windows.</li> <li>If needed: shut down WSL to make sure the changes apply:</li> </ol> <pre><code>wsl --shutdown\n</code></pre>"},{"location":"server/apps/wut/#installing-wut","title":"Installing Wut","text":"<ul> <li>Start a WSL terminal.</li> <li>Install <code>pipx</code>, as recommended by the following <code>wut</code> instructions:</li> </ul> <pre><code>sudo apt update\nsudo apt install pipx\npipx ensurepath\n</code></pre> <ul> <li>Re-launch your terminal to make sure <code>pipx</code> is available, then install <code>wut</code>:</li> </ul> <pre><code>pipx install wut-cli\n</code></pre> <ul> <li>Add <code>wut</code>'s required environment variables to your <code>.bashrc</code> file:</li> </ul> <pre><code>export OPENAI_API_KEY=\"-\"\nexport OPENAI_MODEL=\"Llama-3.2-3B-Instruct-Hybrid\"\nexport OPENAI_BASE_URL=\"http://localhost:8000/api/v1\"\n</code></pre>"},{"location":"server/apps/wut/#usage","title":"Usage","text":""},{"location":"server/apps/wut/#start-a-terminal","title":"Start a terminal","text":"<ol> <li>Start a WSL terminal.</li> <li>Start a <code>tmux</code> session:</li> </ol> <pre><code>tmux\n</code></pre> <p>Then, try some of these example commands that <code>wut</code> can help explain.</p>"},{"location":"server/apps/wut/#help-with-lemonade-server","title":"Help with Lemonade Server","text":"<p>People often ask exactly what Lemonade Server's <code>models</code> endpoint does. Fortunately, <code>wut</code> is able to intuit the answer!</p> <pre><code>curl http://localhost:8000/api/v1/models\nwut\n</code></pre> <p>The terminal response of the <code>curl</code> command is this (only intelligible by machines):</p> <pre><code>curl http://localhost:8000/api/v1/models\n{\"object\":\"list\",\"data\":[{\"id\":\"Qwen2.5-0.5B-Instruct-CPU\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Llama-3.2-1B-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Llama-3.2-3B-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Phi-3-Mini-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Qwen-1.5-7B-Chat-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"DeepSeek-R1-Distill-Llama-8B-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"DeepSeek-R1-Distill-Qwen-7B-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"}]}\n</code></pre> <p>But <code>wut</code> does a nice job interpreting:</p> <pre><code>The output suggests that the API endpoint is returning a list of models, and the owned_by field indicates that all models are owned by \"lemonade\". Thecreated timestamp indicates when each model was created.\n\nThe output is a valid JSON response, and there is no error or warning message. The command was successful, and the output can be used for further processing or analysis. \n</code></pre>"},{"location":"server/apps/wut/#bad-git-command","title":"Bad Git Command","text":"<p>Run a command that doesn't exist, and then ask <code>wut</code> for help:</p> <pre><code>git pull-request\nwut\n</code></pre> <p>Results in:</p> <p>git: 'pull-request' is not a git command. See 'git --help'.</p> <p>And then <code>wut</code> provides some helpful feedback:</p> <p>Key takeaway: The command git pull-request is not a valid Git command. The correct command to create a pull request is git request-pull, but it's not a standard Git command. The output wut is the name of the activated Conda environment. To create a pull request, use git request-pull or git pull with the --pr option. </p>"}]}