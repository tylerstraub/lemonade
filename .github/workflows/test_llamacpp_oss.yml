name: Test Lemonade with Llamacpp ðŸŒ©ï¸

# Note: STX Halo / ROCm / Ubuntu jobs are disabled until we can
# add the required runners

on:
  push:
    branches: ["main"]
  pull_request:
    branches:
      - '**'
  workflow_dispatch:


permissions:
  contents: read

jobs:
  test-server-llamacpp-windows:
    env:
      LEMONADE_CI_MODE: "True"
      CONDA_ENV_PATH: ./lemon-ci-env
      LEMONADE_CACHE_DIR: ./ci-cache

    strategy:
      matrix:
        include:
          - backend: vulkan
            runner: [stx, Windows]
          - backend: rocm
            runner: [stx-halo, Windows]

    runs-on: ${{ matrix.runner }}
    defaults:
      run:
        shell: powershell

    steps:
      - uses: actions/checkout@v3

      - name: Cleanup Python
        shell: PowerShell
        run: |
          # Make sure the system-level python is clean
          Remove-Item -Path "C:\\windows\\system32\\config\\systemprofile\\AppData\\Roaming\\Python" -Recurse -Force

      - name: Set up Conda environment
        run: |
          conda create -p $CONDA_ENV_PATH python=3.10 -y
          conda init

      - name: Install dependencies
        run: |
          conda activate $CONDA_ENV_PATH
          python -m pip install --upgrade pip
          conda install -y pylint
          pip install -e .[llm] --extra-index-url https://pypi.amd.com/simple
          pip check

          echo "HF_HOME=$(pwd)/hf-cache" >> $GITHUB_ENV

      - name: Run lemonade Llamacpp CLI tests
        env:
          HF_HOME: "${{ env.HF_HOME }}"
        run: |
          conda activate $CONDA_ENV_PATH

          if ("${{ matrix.backend }}" -ne "rocm") {
            lemonade -i unsloth/Qwen3-0.6B-GGUF:Q4_0 llamacpp-load --backend ${{ matrix.backend }} --device cpu llm-prompt -p "Hi" --max-new-tokens 10; if ($LASTEXITCODE -ne 0) { exit 1 }
          }

          lemonade -i unsloth/Qwen3-0.6B-GGUF:Q4_0 llamacpp-load --backend ${{ matrix.backend }} --device igpu llm-prompt -p "Hi" --max-new-tokens 10; if ($LASTEXITCODE -ne 0) { exit 1 }
          lemonade -i unsloth/Qwen3-0.6B-GGUF:Q4_0 llamacpp-load --backend ${{ matrix.backend }} llamacpp-bench -w 0 -i 2; if ($LASTEXITCODE -ne 0) { exit 1 }

      - name: Run lemonade Llamacpp API tests
        env:
          HF_HOME: "${{ env.HF_HOME }}"
        run: |
          conda activate $CONDA_ENV_PATH

          python test/server_llamacpp.py ${{ matrix.backend }}; if ($LASTEXITCODE -ne 0) { exit 1 }
          python test/server_llamacpp.py ${{ matrix.backend }} --offline; if ($LASTEXITCODE -ne 0) { exit 1 }

  test-server-llamacpp-ubuntu:
    env:
      LEMONADE_CI_MODE: "True"
      CONDA_ENV_PATH: ./lemon-ci-env
      LEMONADE_CACHE_DIR: ./ci-cache

    strategy:
      matrix:
        include:
          - backend: rocm
            runner: [stx-halo, Linux]
          - backend: vulkan
            runner: [stx, Linux]

    runs-on: ${{ matrix.runner }}
    defaults:
      run:
        shell: bash

    steps:
      - uses: actions/checkout@v3

      - name: Set up Conda environment
        run: |
          conda create -p $CONDA_ENV_PATH python=3.10 -y
          conda init

      - name: Install dependencies
        run: |
          source ~/miniforge3/etc/profile.d/conda.sh
          conda activate $CONDA_ENV_PATH
          python -m pip install --upgrade pip
          conda install -y pylint
          pip install -e .[llm]
          pip check

          echo "HF_HOME=$(pwd)/hf-cache" >> $GITHUB_ENV

      - name: Run lemonade Llamacpp CLI tests
        env:
          HF_HOME: "${{ env.HF_HOME }}"
        run: |
          source ~/miniforge3/etc/profile.d/conda.sh
          conda activate $CONDA_ENV_PATH

          if [ "${{ matrix.backend }}" != "rocm" ]; then
            lemonade -i unsloth/Qwen3-0.6B-GGUF:Q4_0 llamacpp-load --backend ${{ matrix.backend }} --device cpu llm-prompt -p "Hi" --max-new-tokens 10 || exit 1
          fi

          lemonade -i unsloth/Qwen3-0.6B-GGUF:Q4_0 llamacpp-load --backend ${{ matrix.backend }} --device igpu llm-prompt -p "Hi" --max-new-tokens 10 || exit 1
          lemonade -i unsloth/Qwen3-0.6B-GGUF:Q4_0 llamacpp-load --backend ${{ matrix.backend }} llamacpp-bench -w 0 -i 2 || exit 1

      - name: Run lemonade Llamacpp API tests
        env:
          HF_HOME: "${{ env.HF_HOME }}"
        run: |
          source ~/miniforge3/etc/profile.d/conda.sh
          conda activate $CONDA_ENV_PATH

          python test/server_llamacpp.py ${{ matrix.backend }} || exit 1
          python test/server_llamacpp.py ${{ matrix.backend }} --offline || exit 1
